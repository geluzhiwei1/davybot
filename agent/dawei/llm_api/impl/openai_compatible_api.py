# Copyright (c) 2025 æ ¼å¾‹è‡³å¾®
# SPDX-License-Identifier: AGPL-3.0-only

"""OpenAIå…¼å®¹å®¢æˆ·ç«¯å®ç°
ä½¿ç”¨aiohttpå®ç°å¼‚æ­¥HTTPè¯·æ±‚ï¼Œæ”¯æŒOpenAI APIæ ¼å¼çš„å„ç§LLMæœåŠ¡
"""

import json
from collections.abc import AsyncGenerator
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from dawei.core import local_context
from dawei.core.error_handler import handle_errors
from dawei.core.errors import LLMError, ValidationError
from dawei.core.metrics import increment_counter
from dawei.entity.lm_messages import (
    AssistantMessage,
    LLMMessage,
    ChatGeneration,
    ChatResult,
    FunctionCall,
    ToolCall,
)
from dawei.entity.stream_message import (
    CompleteMessage,
    ContentMessage,
    ReasoningMessage,
    StreamMessages,
    ToolCallMessage,
    UsageMessage,
)
from dawei.llm_api.base_client import BaseClient
from dawei.llm_api.base_llm_api import StreamChunkParser
from dawei.logg.logging import get_logger, log_performance

from .stream_processor import StreamProcessor

logger = get_logger(__name__)


class OpenaiCompatibleClient(BaseClient):
    """OpenAIå…¼å®¹çš„LLMå®¢æˆ·ç«¯ï¼Œå®ç°LLMProvideræ¥å£
    ä½¿ç”¨aiohttpå®ç°å¼‚æ­¥HTTPè¯·æ±‚ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”

    è¯¥ç±»ç›´æ¥å®ç°äº†LLMProvideræ¥å£ï¼Œæä¾›äº†ä¸Taskæ¨¡å—å…¼å®¹çš„APIï¼Œ
    åŒæ—¶ä¿æŒäº†åŸæœ‰çš„generate()å’Œstream_generate()æ–¹æ³•ä»¥ç¡®ä¿å‘åå…¼å®¹æ€§ã€‚
    """

    def __init__(self, config: dict[str, Any]) -> None:
        """åˆå§‹åŒ–OpenAIå…¼å®¹å®¢æˆ·ç«¯

        Args:
            config: LLMé…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - apiProvider: æä¾›å•†ç±»å‹ï¼ˆopenaiã€ollamaã€geminiã€deepseekç­‰ï¼‰
                - openAiBaseUrl: OpenAIå…¼å®¹APIçš„åŸºç¡€URL
                - openAiApiKey: APIå¯†é’¥
                - openAiModelId: æ¨¡å‹ID
                - temperature: æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤0.7ï¼‰
                - max_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
                - timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤180ç§’ï¼‰
                - maxRetries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰
                - retryDelay: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆé»˜è®¤1.0ç§’ï¼‰
                - reasoningEffort: æ¨ç†åŠªåŠ›ç¨‹åº¦ï¼ˆå¯é€‰ï¼‰
                - openAiLegacyFormat: æ˜¯å¦ä½¿ç”¨æ—§æ ¼å¼ï¼ˆé»˜è®¤Falseï¼‰
                - openAiCustomModelInfo: è‡ªå®šä¹‰æ¨¡å‹ä¿¡æ¯
                - openAiHeaders: è‡ªå®šä¹‰è¯·æ±‚å¤´

        """
        super().__init__(config)

        # åˆå§‹åŒ–loggerå±æ€§
        self.logger = logger

        self.provider = config.get("apiProvider", "openai")

        # æ ¹æ®providerè·å–é…ç½®
        if self.provider == "ollama":
            self.base_url = config.get("ollamaBaseUrl", "http://localhost:11434").rstrip("/")
            self.api_key = config.get("ollamaApiKey", "ollama")
            self.model = config.get("ollamaModelId", "llama3.1")
        else:  # é»˜è®¤ä¸ºopenaiå…¼å®¹
            # æ”¯æŒä¸¤ç§å­—æ®µåï¼šopenAiBaseUrlï¼ˆåŸå§‹é…ç½®ï¼‰å’Œ base_urlï¼ˆLLMConfigå†…éƒ¨å­—æ®µï¼‰
            self.base_url = config.get("openAiBaseUrl", config.get("base_url", "")).rstrip("/")
            # æ”¯æŒä¸¤ç§å­—æ®µåï¼šopenAiApiKeyï¼ˆåŸå§‹é…ç½®ï¼‰å’Œ api_keyï¼ˆLLMConfigå†…éƒ¨å­—æ®µï¼‰
            self.api_key = config.get("openAiApiKey", config.get("api_key", ""))
            # æ”¯æŒä¸¤ç§å­—æ®µåï¼šopenAiModelIdï¼ˆåŸå§‹é…ç½®ï¼‰å’Œ model_idï¼ˆLLMConfigå†…éƒ¨å­—æ®µï¼‰
            self.model = config.get("openAiModelId", config.get("model_id", ""))

        # éªŒè¯é…ç½® - ä½¿ç”¨æ–­è¨€å¼éªŒè¯æ›¿ä»£assert
        if not self.base_url or not self.base_url.startswith("http"):
            raise ValidationError("base_url", self.base_url, "must be a valid HTTP/HTTPS URL")

        # roo code ç‰¹æœ‰é…ç½®
        self.reasoning_effort = config.get("reasoningEffort", config.get("reasoning_effort"))
        self.legacy_format = config.get("openAiLegacyFormat", False)
        # æ”¯æŒä¸¤ç§å­—æ®µåï¼šopenAiCustomModelInfoï¼ˆåŸå§‹é…ç½®ï¼‰å’Œ custom_model_infoï¼ˆLLMConfigå†…éƒ¨å­—æ®µï¼‰
        self.custom_model_info = config.get(
            "openAiCustomModelInfo",
            config.get("custom_model_info", {}),
        )
        self.max_context_tokens = self.custom_model_info.get("max_context_tokens")
        self.max_output_tokens = self.custom_model_info.get("max_output_tokens")

        # ç§»é™¤tiktokenä¾èµ–ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°ä¼°ç®—
        self.encoding = None

        # è®¾ç½®é»˜è®¤å‚æ•°
        self.default_params = {
            "model": self.model,
            "temperature": config.get("temperature", 0.7),
            # "max_tokens": config.get("max_tokens"),
            # "top_p": config.get("top_p", 1.0),
            # "frequency_penalty": config.get("frequency_penalty", 0.0),
            # "presence_penalty": config.get("presence_penalty", 0.0),
            "stream": False,
        }

    def _prepare_headers(self) -> dict[str, str]:
        """å‡†å¤‡è¯·æ±‚å¤´"""
        headers = {
            "Content-Type": "application/json",
            # Mimic headers sent by the VSCode extension
            "HTTP-Referer": "https://github.com/RooVetGit/Roo-Cline",
            "X-Title": "Roo Code",
            "User-Agent": "RooCode/3.38.0",  # Hardcoded version for testing
        }

        # roo code è‡ªå®šä¹‰headers
        # æ”¯æŒä¸¤ç§å­—æ®µåï¼šopenAiHeadersï¼ˆåŸå§‹é…ç½®ï¼‰å’Œ custom_headersï¼ˆLLMConfigå†…éƒ¨å­—æ®µï¼‰
        custom_headers = self.config.get("openAiHeaders", self.config.get("custom_headers"))
        if custom_headers and isinstance(custom_headers, dict):
            headers.update(custom_headers)

        # è®¤è¯
        if "Authorization" not in headers:
            if self.provider == "gemini":
                # Gemini uses API key in URL, not header
                pass
            elif self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

        return headers

    @handle_errors(component="openai_compatible_api", operation="prepare_log_files")
    def _prepare_log_files(
        self,
        workspace_path: str,
        _endpoint: str,
    ) -> tuple[Path | None, Path | None]:
        """å‡†å¤‡æ—¥å¿—æ–‡ä»¶è·¯å¾„

        Args:
            workspace_path: å·¥ä½œç©ºé—´è·¯å¾„
            endpoint: APIç«¯ç‚¹

        Returns:
            (request_log_file, response_log_file): æ—¥å¿—æ–‡ä»¶è·¯å¾„å…ƒç»„

        """
        if not workspace_path:
            return None, None

        log_dir = Path(workspace_path) / ".dawei" / "http"
        log_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_%f")
        request_log_file = log_dir / f"{timestamp}_request.json"
        response_log_file = log_dir / f"{timestamp}_response.json"

        return request_log_file, response_log_file

    def _log_request(self, request_log_file: Path, endpoint: str, params: dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—

        Args:
            request_log_file: è¯·æ±‚æ—¥å¿—æ–‡ä»¶è·¯å¾„
            endpoint: APIç«¯ç‚¹
            params: è¯·æ±‚å‚æ•°

        """
        if not request_log_file:
            return

        request_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "url": f"{self.base_url}/{endpoint}",
            "headers": dict(self._prepare_headers()),
            "params": params,
            "provider": self.provider,
            "model": self.model,
        }

        with request_log_file.open("w", encoding="utf-8") as f:
            json.dump(request_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Request logged to: {request_log_file}")

    @handle_errors(component="openai_compatible_api", operation="execute_http_request")
    @log_performance("openai_compatible_api.execute_http_request")
    async def _execute_http_request(
        self,
        workspace_path: str,
        response_log_file: Path | None,
        endpoint: str,
        params: dict[str, Any],
        _url_suffix: str,
        _original_url: str,
    ) -> tuple[AsyncGenerator[bytes, None], int | None, dict[str, str] | None, str | None]:
        """æ‰§è¡ŒHTTPè¯·æ±‚å¹¶è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨ï¼ˆå¸¦ä¿æŠ¤å±‚ï¼‰

        é›†æˆäº†é€Ÿç‡é™åˆ¶å™¨ã€æ–­è·¯å™¨å’Œç›‘æ§æŒ‡æ ‡

        Args:
            workspace_path: å·¥ä½œç©ºé—´è·¯å¾„
            response_log_file: å“åº”æ—¥å¿—æ–‡ä»¶è·¯å¾„
            endpoint: APIç«¯ç‚¹
            params: è¯·æ±‚å‚æ•°
            url_suffix: URLåç¼€
            original_url: åŸå§‹URL

        Returns:
            (response_stream, response_status, response_headers, url): å“åº”æµç”Ÿæˆå™¨å…ƒç»„

        """
        # âœ… è·å–æ–­è·¯å™¨
        circuit_breaker = self._get_circuit_breaker()
        url = f"{self.base_url}/{endpoint}"
        provider = self.get_provider_name()
        model = getattr(self, "model", "unknown")

        # âœ… å®šä¹‰å—ä¿æŠ¤çš„æµå¼è¯·æ±‚å‡½æ•°
        async def make_protected_streaming_request():
            # 0. é˜²å¾¡æ€§æƒ°æ€§åˆå§‹åŒ–
            BaseClient._ensure_rate_limiter(self.config)

            # 1. é€Ÿç‡é™åˆ¶æ£€æŸ¥
            success, wait_time = await BaseClient._global_rate_limiter.acquire(timeout=30.0)

            if not success:
                raise Exception(f"Rate limit exceeded, wait time: {wait_time:.1f}s")

            logger.info(f"âœ… [RATE_LIMITER] Acquired token for {provider}:{model}")

            try:
                # 2. æ‰§è¡Œå®é™…çš„æµå¼è¯·æ±‚
                session = self._get_client_session()

                async def response_stream_generator():
                    """å†…éƒ¨æµå¼å“åº”ç”Ÿæˆå™¨"""
                    response_chunks_for_logging = []
                    response_status_local = None
                    response_headers_local = None
                    url_local = url

                    try:
                        async with session.post(url, json=params) as response:
                            response_status_local = response.status
                            response_headers_local = dict(response.headers)

                            if response.status != 200:
                                error_text = await response.text()

                                # è®°å½•é”™è¯¯å“åº”æ—¥å¿—
                                if workspace_path and response_log_file:
                                    self._log_error_response(
                                        response_log_file,
                                        response_status_local,
                                        response_headers_local,
                                        error_text,
                                        url_local,
                                    )

                                raise LLMError(
                                    self.provider,
                                    f"HTTP {response.status}: {error_text}",
                                )

                            # å®æ—¶æµå¼å¤„ç†ï¼šç«‹å³ yield æ¯ä¸ªæ•°æ®å—ï¼ŒåŒæ—¶æ”¶é›†ç”¨äºæ—¥å¿—è®°å½•
                            async for chunk in response.content:
                                # ç«‹å³ yield æ•°æ®å—ä»¥å®ç°å®æ—¶æµå¼å¤„ç†
                                yield chunk
                                # åŒæ—¶æ”¶é›†æ•°æ®å—ç”¨äºåç»­æ—¥å¿—è®°å½•
                                response_chunks_for_logging.append(chunk)

                    finally:
                        # ç¡®ä¿å“åº”æ—¥å¿—ä¸€å®šè¢«è®°å½•ï¼Œæ— è®ºæµæ˜¯å¦å®Œå…¨æ¶ˆè´¹
                        if workspace_path and response_log_file:
                            self._log_response(
                                response_log_file,
                                response_chunks_for_logging,
                                response_status_local,
                                response_headers_local,
                                url_local,
                            )

                        self.logger.info(
                            "HTTP request completed",
                            context={
                                "endpoint": endpoint,
                                "status": response_status_local,
                                "provider": self.provider,
                                "component": "openai_compatible_api",
                            },
                        )
                        increment_counter(
                            "openai_compatible_api.http_requests",
                            tags={
                                "provider": self.provider,
                                "status": "success" if response_status_local == 200 else "error",
                            },
                        )

                return response_stream_generator(), None, None, url

            except Exception as e:
                # 3. è®°å½•å¤±è´¥
                error_str = str(e)
                is_rate_limit = "429" in error_str or "rate_limit" in error_str.lower()
                BaseClient._global_rate_limiter.record_failure(is_rate_limit=is_rate_limit)
                logger.exception("âŒ [RATE_LIMITER] Request failed for {provider}:{model}: ")
                raise

        # âœ… é€šè¿‡æ–­è·¯å™¨è°ƒç”¨ï¼ˆè‡ªåŠ¨é‡è¯•ï¼‰
        logger.info(f"ğŸ”„ [CIRCUIT_BREAKER] Executing streaming request for {provider}:{model}")
        result = await circuit_breaker.call(make_protected_streaming_request)

        # âœ… è®°å½•æˆåŠŸ
        BaseClient._global_rate_limiter.record_success()
        logger.info(f"âœ… [RATE_LIMITER] Request completed successfully for {provider}:{model}")

        return result

    def _log_error_response(
        self,
        response_log_file: Path,
        status: int,
        headers: dict[str, str],
        error_text: str,
        url: str,
    ) -> None:
        """è®°å½•é”™è¯¯å“åº”æ—¥å¿—

        Args:
            response_log_file: å“åº”æ—¥å¿—æ–‡ä»¶è·¯å¾„
            status: HTTPçŠ¶æ€ç 
            headers: å“åº”å¤´
            error_text: é”™è¯¯æ–‡æœ¬
            url: è¯·æ±‚URL

        Raises:
            OSError: æ–‡ä»¶å†™å…¥å¤±è´¥
            ValueError: æ•°æ®åºåˆ—åŒ–å¤±è´¥

        """
        error_response_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "status": status,
            "headers": headers,
            "error": error_text,
            "url": url,
        }

        with response_log_file.open("w", encoding="utf-8") as f:
            json.dump(error_response_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Error response logged to: {response_log_file}")

    def _log_response(
        self,
        response_log_file: Path,
        response_chunks: list[bytes],
        status: int | None,
        headers: dict[str, str] | None,
        url: str | None,
    ) -> None:
        """è®°å½•å“åº”æ—¥å¿—

        Args:
            response_log_file: å“åº”æ—¥å¿—æ–‡ä»¶è·¯å¾„
            response_chunks: å“åº”æ•°æ®å—åˆ—è¡¨
            status: HTTPçŠ¶æ€ç 
            headers: å“åº”å¤´
            url: è¯·æ±‚URL

        Raises:
            OSError: æ–‡ä»¶å†™å…¥å¤±è´¥
            ValueError: æ•°æ®åºåˆ—åŒ–å¤±è´¥

        """
        if response_chunks:
            # æœ‰å“åº”æ•°æ®æ—¶çš„æ—¥å¿—
            combined_bytes = b"".join(response_chunks)
            response_text = combined_bytes.decode("utf-8", errors="ignore")

            response_data = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "status": status or "unknown",
                "headers": headers or {},
                "url": url or "unknown",
                "raw_response": response_text,
                "chunks_count": len(response_chunks),
                "total_bytes": len(combined_bytes),
            }
        else:
            # æ²¡æœ‰å“åº”æ•°æ®æ—¶çš„æ—¥å¿—
            response_data = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "status": status or "unknown",
                "headers": headers or {},
                "url": url or "unknown",
                "raw_response": "",
                "chunks_count": 0,
                "total_bytes": 0,
                "note": "No response chunks captured",
            }

        with response_log_file.open("w", encoding="utf-8") as f:
            json.dump(response_data, f, indent=2, ensure_ascii=False)
        logger.info(f"Response logged to: {response_log_file}")

    def get_num_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ä»¤ç‰Œæ•°é‡ï¼ˆä¸ä½¿ç”¨tiktokenï¼‰"""
        if not text:
            return 0
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°ä¼°ç®—ï¼šå¹³å‡4ä¸ªå­—ç¬¦çº¦ç­‰äº1ä¸ªtoken
        # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥çš„ä¼°ç®—ï¼Œå¯¹äºä¸­æ–‡å¯èƒ½ä¸å¤ªå‡†ç¡®ï¼Œä½†è¶³å¤Ÿç”¨äºåŸºæœ¬çš„tokené™åˆ¶
        return len(text) // 4

    def get_num_tokens_from_messages(self, messages: list[dict[str, Any]]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»ä»¤ç‰Œæ•°"""
        total_tokens = 0
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, list):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹æ ¼å¼
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content = item.get("text", "")
                        total_tokens += self.get_num_tokens(text_content)
            else:
                # å¤„ç†çº¯æ–‡æœ¬å†…å®¹
                total_tokens += self.get_num_tokens(content)
        return total_tokens

    def _prepare_request_params(self, messages: list[LLMMessage], **kwargs) -> dict[str, Any]:
        """å‡†å¤‡è¯·æ±‚å‚æ•°"""
        params = {**self.default_params, **kwargs}

        # è½¬æ¢æ¶ˆæ¯ä¸ºå­—å…¸æ ¼å¼ï¼Œç¡®ä¿JSONåºåˆ—åŒ–å…¼å®¹
        serialized_messages = []
        for message in messages:
            if hasattr(message, "to_dict"):
                serialized_messages.append(message.to_dict())
            else:
                serialized_messages.append(message)

        params["messages"] = serialized_messages

        # if params.get("top_p") == 1.0:
        #     del params["top_p"]
        # if params.get("frequency_penalty") == 0.0:
        #     del params["frequency_penalty"]
        # if params.get("presence_penalty") == 0.0:
        #     del params["presence_penalty"]

        params["stream"] = True
        params["tool_stream"] = True

        # # æ·»åŠ  reasoning_effort
        # if self.reasoning_effort is not None:
        #     params["reasoning_effort"] = self.reasoning_effort

        # è‡ªåŠ¨è°ƒæ•´ max_tokens
        # if self.max_context_tokens and params.get("max_tokens") is None:
        #     prompt_tokens = self.get_num_tokens_from_messages(openai_messages)
        #     available_tokens = self.max_context_tokens - prompt_tokens

        #     available_tokens -= 100  # Buffer

        #     if self.max_output_tokens:
        #         params["max_tokens"] = min(available_tokens, self.max_output_tokens)
        #     else:
        #         params["max_tokens"] = max(256, available_tokens)

        #     if params["max_tokens"] <= 0:
        #         raise ValueError("Prompt is too long, not enough tokens for generation.")

        return params

    def _convert_response_to_chat_result(self, response_data: dict[str, Any]) -> ChatResult:
        """å°†APIå“åº”è½¬æ¢ä¸ºChatResult"""
        choices = response_data.get("choices", [])
        generations = []

        for choice in choices:
            message_data = choice.get("message", {})
            content = message_data.get("content", "")

            ai_message = AssistantMessage(content=content)

            generation = ChatGeneration(
                message=ai_message,
                text=content,
                finish_reason=choice.get("finish_reason"),
                generation_info=choice.get("logprobs"),
            )
            generations.append(generation)

        return ChatResult(
            generations=generations,
            llm_output=({"model_name": response_data.get("model")} if isinstance(response_data.get("model"), str) else response_data.get("model")),
            usage=response_data.get("usage"),
            response_metadata=response_data,
        )

    @handle_errors(component="openai_compatible_api", operation="make_http_request")
    @log_performance("openai_compatible_api.make_http_request")
    async def _make_http_request(self, endpoint: str, params: dict[str, Any]) -> dict[str, Any]:
        """æ‰§è¡Œå•æ¬¡HTTPè¯·æ±‚"""
        # å¤„ç† Gemini ç‰¹æ®Šæƒ…å†µ
        url_suffix = f"?key={self.api_key}" if self.provider == "gemini" else ""
        original_url = self.base_url
        if url_suffix:
            self.base_url = self.base_url + url_suffix

        # ç›´æ¥è°ƒç”¨ï¼Œç§»é™¤å†—ä½™é”™è¯¯å¤„ç†
        result = await super()._make_http_request(endpoint, params)

        # æ¢å¤åŸå§‹ base_url
        if url_suffix:
            self.base_url = original_url

        self.logger.info(
            "HTTP request completed",
            context={
                "endpoint": endpoint,
                "provider": self.provider,
                "component": "openai_compatible_api",
            },
        )
        increment_counter(
            "openai_compatible_api.http_requests",
            tags={"provider": self.provider, "status": "success"},
        )

        return result

    @handle_errors(component="openai_compatible_api", operation="create_message")
    @log_performance("openai_compatible_api.create_message")
    async def create_message(
        self,
        messages: list[LLMMessage],
        **kwargs,
    ) -> AsyncGenerator[StreamMessages, None]:
        """åˆ›å»ºæ¶ˆæ¯ï¼Œæ”¯æŒæ¨ç†è¿‡ç¨‹ã€å†…å®¹å’Œå·¥å…·è°ƒç”¨çš„åˆ†åˆ«å¤„ç†"""
        # éªŒè¯è¾“å…¥
        if not messages:
            raise ValidationError("messages", messages, "must be non-empty list")

        # å‡†å¤‡è¯·æ±‚å‚æ•°
        params = self._prepare_request_params(messages, stream=True, **kwargs)

        self.logger.info(
            "Creating message",
            context={
                "model": self.model,
                "message_count": len(messages),
                "component": "openai_compatible_api",
            },
        )

        endpoint = "chat/completions"
        if self.provider == "ollama":
            endpoint = "api/chat"

        # åˆ›å»ºåŸå§‹æµæ•°æ®ç”Ÿæˆå™¨
        async def raw_stream_generator():
            # å¤„ç† Gemini ç‰¹æ®Šæƒ…å†µ
            url_suffix = f"?key={self.api_key}" if self.provider == "gemini" else ""
            original_url = self.base_url
            if url_suffix:
                self.base_url = self.base_url + url_suffix

            # å‡†å¤‡æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶
            workspace_path = self.config.get("workspace_path", "")
            request_log_file, response_log_file = self._prepare_log_files(workspace_path, endpoint)

            # è®°å½•è¯·æ±‚æ—¥å¿—
            if request_log_file:
                self._log_request(request_log_file, endpoint, params)

            # æ‰§è¡ŒHTTPè¯·æ±‚å¹¶è·å–æµå¼å“åº”ç”Ÿæˆå™¨
            (
                response_stream,
                _response_status,
                _response_headers,
                _url,
            ) = await self._execute_http_request(
                workspace_path,
                response_log_file,
                endpoint,
                params,
                url_suffix,
                original_url,
            )

            # æ¢å¤åŸå§‹ base_url
            if url_suffix:
                self.base_url = original_url

            # å®æ—¶æµå¼å¤„ç†ï¼šç›´æ¥ yield æµå¼å“åº”ä¸­çš„æ¯ä¸ªæ•°æ®å—
            async for chunk in response_stream:
                yield chunk

        # ä½¿ç”¨æµå¼å¤„ç†å™¨å¤„ç†æ•°æ®å¹¶ç›´æ¥è¿”å› BaseStreamMessage
        stream_processor = StreamProcessor(provider=self.provider)
        async for message in stream_processor.process_stream(raw_stream_generator()):
            yield message

        increment_counter(
            "openai_compatible_api.messages_created",
            tags={"provider": self.provider, "message_count": len(messages)},
        )

    @handle_errors(component="openai_compatible_api", operation="astream_chat_completion")
    @log_performance("openai_compatible_api.astream_chat_completion")
    async def astream_chat_completion(
        self,
        messages: list[LLMMessage],
        **kwargs,
    ) -> AsyncGenerator[StreamMessages, None]:
        """å¼‚æ­¥æµå¼èŠå¤©å®Œæˆï¼Œå…¼å®¹æ—§æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æµå¼å“åº”ç”Ÿæˆå™¨

        """
        # éªŒè¯è¾“å…¥
        if not messages:
            raise ValidationError("messages", messages, "must be non-empty list")

        # ç›´æ¥è°ƒç”¨ create_message æ–¹æ³•ä»¥ä¿æŒå…¼å®¹æ€§
        async for message in self.create_message(messages, **kwargs):
            yield message

        increment_counter(
            "openai_compatible_api.stream_chat_completions",
            tags={"provider": self.provider, "message_count": len(messages)},
        )


class OpenAICompatibleParser(StreamChunkParser):
    """OpenAIå…¼å®¹APIçš„æµå¼æ•°æ®è§£æå™¨ï¼Œæ¯æ¬¡è¯·æ±‚ä½¿ç”¨æ–°çš„å®ä¾‹"""

    def __init__(self):
        self.reasoning_content = ""
        self.content = ""
        self.final_tool_calls = {}
        self.tool_call_buffers = {}  # ä¸ºæ¯ä¸ªå·¥å…·è°ƒç”¨ç»´æŠ¤å‚æ•°ç¼“å†²åŒº
        self.reasoning_started = False
        self.content_started = False
        self.final_usage = None

    def parse_chunk(self, chunk: dict[str, Any]) -> list[StreamMessages]:
        """è§£æOpenAIå…¼å®¹æ ¼å¼çš„æ•°æ®å—"""
        messages = []

        # è§£æ chunk é¡¶å±‚å­—æ®µ
        chunk_id = chunk.get("id")
        chunk_created = chunk.get("created")
        chunk_object = chunk.get("object")
        chunk_model = chunk.get("model")

        # æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯
        if chunk.get("usage"):
            usage_data = chunk.get("usage", {})
            self.final_usage = usage_data
            messages.append(
                UsageMessage(
                    user_message_id=local_context.get_message_id(),
                    data=usage_data,
                    id=chunk_id,
                    created=chunk_created,
                    object=chunk_object,
                    model=chunk_model,
                ),
            )

        if not chunk.get("choices"):
            return messages

        delta = chunk["choices"][0].get("delta", {})

        # ã€DEBUGã€‘è®°å½•æ”¶åˆ°çš„åŸå§‹deltaï¼Œç”¨äºè°ƒè¯•GLMç­‰æ¨¡å‹çš„å†…å®¹æ˜¾ç¤ºé—®é¢˜
        if delta:  # åªåœ¨deltaéç©ºæ—¶è®°å½•
            logger.info(f"[LLM_STREAM] Received delta: keys={list(delta.keys())}, has_reasoning={bool(delta.get('reasoning_content'))}, has_content={bool(delta.get('content'))}, has_tool_calls={bool(delta.get('tool_calls'))}")
            if delta.get("reasoning_content"):
                logger.info(f"[LLM_STREAM] reasoning_content preview: {delta['reasoning_content'][:100]!r}")
            if delta.get("content"):
                logger.info(f"[LLM_STREAM] content preview: {delta['content'][:100]!r}")

        # å¤„ç†æµå¼æ¨ç†è¿‡ç¨‹è¾“å‡º
        if delta.get("reasoning_content"):
            reasoning_delta = delta["reasoning_content"]
            # ã€å…³é”®ä¿®å¤ã€‘åªå¤„ç†æœ‰å®é™…å†…å®¹çš„éƒ¨åˆ†ï¼Œå®Œå…¨è·³è¿‡ç©ºç™½æˆ–ä»…åŒ…å«ç©ºç™½å­—ç¬¦çš„å†…å®¹
            # è¿‡æ»¤æ‰åªæœ‰ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ç©ºç™½å­—ç¬¦çš„å†…å®¹
            if reasoning_delta and reasoning_delta.strip():
                if not self.reasoning_started:
                    self.reasoning_started = True
                self.reasoning_content += reasoning_delta

                messages.append(
                    ReasoningMessage(
                        user_message_id=local_context.get_message_id(),
                        content=reasoning_delta,
                        id=chunk_id,
                        created=chunk_created,
                        object=chunk_object,
                        model=chunk_model,
                    ),
                )

                # ã€å…³é”®ä¿®å¤ã€‘å¦‚æœè¿˜æ²¡æœ‰contentï¼Œä¹Ÿåˆ›å»ºContentMessageä»¥ç¡®ä¿assistantæ°”æ³¡èƒ½æ˜¾ç¤º
                # è¿™æ˜¯ä¸ºäº†å¤„ç†GLMç­‰æ¨¡å‹å°†æ‰€æœ‰æ–‡æœ¬æ”¾åœ¨reasoning_contentä¸­çš„æƒ…å†µ
                if not self.content:
                    self.content += reasoning_delta
                    messages.append(
                        ContentMessage(
                            user_message_id=local_context.get_message_id(),
                            content=reasoning_delta,
                            id=chunk_id,
                            created=chunk_created,
                            object=chunk_object,
                            model=chunk_model,
                        ),
                    )
            # è®°å½•è¢«è¿‡æ»¤çš„ç©ºæ¨ç†å†…å®¹ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
            elif reasoning_delta:
                logger.debug(f"Filtered empty reasoning content: {reasoning_delta!r}")

        # å¤„ç†æµå¼å›ç­”å†…å®¹è¾“å‡º
        if delta.get("content"):
            content_delta = delta["content"]
            # ã€å…³é”®ä¿®å¤ã€‘åªå¤„ç†æœ‰å®é™…å†…å®¹çš„éƒ¨åˆ†ï¼Œå®Œå…¨è·³è¿‡ç©ºç™½æˆ–ä»…åŒ…å«ç©ºç™½å­—ç¬¦çš„å†…å®¹
            # è¿‡æ»¤æ‰åªæœ‰ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ç©ºç™½å­—ç¬¦çš„å†…å®¹
            if content_delta and content_delta.strip():
                if not self.content_started:
                    self.content_started = True
                self.content += content_delta

                messages.append(
                    ContentMessage(
                        user_message_id=local_context.get_message_id(),
                        content=content_delta,
                        id=chunk_id,
                        created=chunk_created,
                        object=chunk_object,
                        model=chunk_model,
                    ),
                )
            # è®°å½•è¢«è¿‡æ»¤çš„ç©ºå†…å®¹ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
            elif content_delta:
                logger.debug(f"Filtered empty content: {content_delta!r}")

        # å¤„ç†æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯
        if delta.get("tool_calls"):
            for tool_call in delta.get("tool_calls", []):
                index = tool_call.get("index")

                # åˆå§‹åŒ–å·¥å…·è°ƒç”¨ç¼“å†²åŒº
                if index not in self.tool_call_buffers:
                    self.tool_call_buffers[index] = {
                        "id": tool_call.get("id", ""),
                        "type": tool_call.get("type", "function"),
                        "function": {
                            "name": tool_call.get("function", {}).get("name", ""),
                            "arguments": "",  # åˆå§‹åŒ–ç©ºçš„å‚æ•°ç¼“å†²åŒº
                        },
                    }
                    self.final_tool_calls[index] = self.tool_call_buffers[index].copy()

                # ç¼“å†²å‚æ•°ç‰‡æ®µ
                arguments_delta = tool_call.get("function", {}).get("arguments", "")
                if arguments_delta:
                    self.tool_call_buffers[index]["function"]["arguments"] += arguments_delta

                # æ›´æ–°å‡½æ•°åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if tool_call.get("function", {}).get("name"):
                    self.tool_call_buffers[index]["function"]["name"] = tool_call.get(
                        "function",
                        {},
                    ).get("name")

                # åŒæ­¥åˆ° final_tool_calls
                self.final_tool_calls[index] = self.tool_call_buffers[index].copy()

                # åˆ›å»ºå½“å‰æ•°æ®å—çš„ ToolCall å¯¹è±¡ï¼Œä½¿ç”¨ç¼“å†²çš„å®Œæ•´å‚æ•°
                current_arguments = self.tool_call_buffers[index]["function"]["arguments"]
                tool_call_obj = ToolCall(
                    tool_call_id=self.tool_call_buffers[index]["id"],
                    type=self.tool_call_buffers[index]["type"],
                    function=FunctionCall(
                        name=self.tool_call_buffers[index]["function"]["name"],
                        arguments=current_arguments,
                    ),
                )

                # å°†æ‰€æœ‰å·¥å…·è°ƒç”¨å­—å…¸è½¬æ¢ä¸º ToolCall å¯¹è±¡åˆ—è¡¨
                all_tool_calls = []
                for tc_dict in self.final_tool_calls.values():
                    all_tool_calls.append(
                        ToolCall(
                            tool_call_id=tc_dict.get("id", ""),
                            type=tc_dict.get("type", "function"),
                            function=FunctionCall(
                                name=tc_dict.get("function", {}).get("name", ""),
                                arguments=tc_dict.get("function", {}).get("arguments", "{}"),
                            ),
                        ),
                    )

                messages.append(
                    ToolCallMessage(
                        user_message_id=local_context.get_message_id(),
                        tool_call=tool_call_obj,
                        all_tool_calls=all_tool_calls,
                        id=chunk_id,
                        created=chunk_created,
                        object=chunk_object,
                        model=chunk_model,
                    ),
                )

        return messages

    def complete(self, chunk: dict[str, Any]) -> CompleteMessage:
        """åˆ›å»ºå®Œæˆæ¶ˆæ¯"""
        # å°†å·¥å…·è°ƒç”¨å­—å…¸è½¬æ¢ä¸º ToolCall å¯¹è±¡åˆ—è¡¨ï¼Œä½¿ç”¨ç¼“å†²çš„å®Œæ•´å‚æ•°
        tool_calls = []
        for tc_dict in self.final_tool_calls.values():
            # ã€å…³é”®ä¿®å¤ã€‘ä¼˜å…ˆä½¿ç”¨ç¼“å†²åŒºä¸­çš„å®Œæ•´å‚æ•°
            arguments = tc_dict.get("function", {}).get("arguments", "{}")
            if not arguments or arguments == "{}":
                # å¦‚æœå‚æ•°ä¸ºç©ºï¼Œå°è¯•ä»ç¼“å†²åŒºè·å–
                index = None
                for idx, buffered_tc in self.tool_call_buffers.items():
                    if buffered_tc["id"] == tc_dict.get("id"):
                        index = idx
                        break
                if index is not None and self.tool_call_buffers[index]["function"]["arguments"]:
                    arguments = self.tool_call_buffers[index]["function"]["arguments"]

            tool_calls.append(
                ToolCall(
                    tool_call_id=tc_dict.get("id", ""),
                    type=tc_dict.get("type", "function"),
                    function=FunctionCall(
                        name=tc_dict.get("function", {}).get("name", ""),
                        arguments=arguments,
                    ),
                ),
            )

        # è·å–å®ŒæˆåŸå› ï¼Œå¦‚æœchunkä¸ºç©ºæˆ–æ²¡æœ‰choicesï¼Œåˆ™é»˜è®¤ä¸º"stop"
        finish_reason = "stop"
        if chunk and chunk.get("choices"):
            finish_reason = chunk["choices"][0].get("finish_reason", "stop")

        # ã€å…³é”®ä¿®å¤ã€‘å½“contentä¸ºç©ºä½†reasoning_contentæœ‰å†…å®¹æ—¶ï¼Œå°†reasoning_contentå¤åˆ¶åˆ°content
        # è¿™æ˜¯ä¸ºäº†å¤„ç†GLMç­‰æ¨¡å‹å°†æ‰€æœ‰æ–‡æœ¬æ”¾åœ¨reasoning_contentä¸­çš„æƒ…å†µ
        final_content = self.content
        if (not final_content or not final_content.strip()) and self.reasoning_content and self.reasoning_content.strip():
            logger.info(
                f"Copying reasoning_content to content as content field is empty. Reasoning length: {len(self.reasoning_content)}",
            )
            final_content = self.reasoning_content

        return CompleteMessage(
            reasoning_content=self.reasoning_content,
            content=final_content,
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            usage=self.final_usage,
        )
