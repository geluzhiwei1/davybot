# Copyright (c) 2025 æ ¼å¾‹è‡³å¾®
# SPDX-License-Identifier: AGPL-3.0-only

"""ä»»åŠ¡æ‰§è¡Œå¼•æ“
è´Ÿè´£ä»»åŠ¡çš„æ‰§è¡Œã€æ¶ˆæ¯å¤„ç†å’Œå·¥å…·è°ƒç”¨
"""

import asyncio
import time
from typing import Any

from dawei.agentic.agent_config import Config
from dawei.agentic.checkpoint_manager import (
    CheckpointType,
    IntelligentCheckpointManager,
)
from dawei.config import get_dawei_home
from dawei.core.errors import CheckpointError, LLMError
from dawei.core.events import TaskEventType, emit_typed_event
from dawei.entity.lm_messages import (
    AssistantMessage,
    LLMMessage,
    MessageRole,
    SystemMessage,
    ToolCall,
    ToolMessage,
    UserMessage,
)
from dawei.entity.stream_message import (
    CompleteMessage,
    ContentMessage,
    ErrorMessage,
    ReasoningMessage,
    StreamMessages,
    ToolCallMessage,
    UsageMessage,
)

# ä½¿ç”¨å†…ç½®çš„ asyncio.timeout (Python 3.11+)
from dawei.entity.task_types import TaskStatus
from dawei.interfaces import IEventBus, ILLMService, IMessageProcessor, IToolCallService
from dawei.logg.logging import get_logger
from dawei.task_graph.task_node import TaskNode
from dawei.task_graph.todo_models import TodoItem, TodoStatus
from dawei.workspace.user_workspace import UserWorkspace

from .tool_message_handler import ToolMessageHandle


class TaskNodeExecutionEngine:
    """ä»»åŠ¡æ‰§è¡Œå¼•æ“å®ç°
    æ‰§è¡Œä¸€ä¸ªnode,æ›´æ–°nodeçš„çŠ¶æ€ï¼Œtodos
    """

    def __init__(
        self,
        task_node: TaskNode,
        user_workspace: UserWorkspace,
        message_processor: IMessageProcessor,
        llm_service: ILLMService,
        tool_call_service: IToolCallService,
        event_bus: IEventBus,
        config: Config,
        agent=None,  # æ·»åŠ agentå¼•ç”¨ç”¨äºæš‚åœæ£€æŸ¥
    ):
        """åˆå§‹åŒ–ä»»åŠ¡æ‰§è¡Œå¼•æ“

        Args:
            task_node: ä»»åŠ¡èŠ‚ç‚¹å®ä¾‹
            user_workspace: ç”¨æˆ·å·¥ä½œåŒºå®ä¾‹
            message_processor: æ¶ˆæ¯å¤„ç†å™¨æ¥å£å®ä¾‹
            llm_service: LLM æœåŠ¡æ¥å£å®ä¾‹
            tool_call_service: å·¥å…·è°ƒç”¨æœåŠ¡æ¥å£å®ä¾‹
            event_bus: äº‹ä»¶æ€»çº¿æ¥å£å®ä¾‹
            config: ç»Ÿä¸€é…ç½®å®ä¾‹
            agent: Agentå®ä¾‹ï¼ˆå¯é€‰ï¼Œç”¨äºæš‚åœ/æ¢å¤æ§åˆ¶ï¼‰

        """
        # ç›´æ¥ä¿å­˜ä»»åŠ¡èŠ‚ç‚¹å¼•ç”¨
        self.task_node = task_node
        self._agent = agent  # ä¿å­˜agentå¼•ç”¨

        # ä¿å­˜å…¶ä»–ä¾èµ–
        self._user_workspace = user_workspace
        self._message_processor = message_processor
        self._llm_service = llm_service
        self._tool_call_service = tool_call_service
        self._event_bus = event_bus
        self._config = config

        # åˆå§‹åŒ–å…¶ä»–å±æ€§
        self.logger = get_logger(__name__)
        self.execution_task: asyncio.Task | None = None
        self.last_checkpoint_time: float = 0.0
        self.last_cleanup_time: float = time.time()  # æ·»åŠ æ¸…ç†æ—¶é—´æˆ³

        # åˆå§‹åŒ– IntelligentCheckpointManagerï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ WorkspacePersistenceManagerï¼‰
        self._checkpoint_manager = IntelligentCheckpointManager(
            workspace_path=self._user_workspace.absolute_path,
        )

        # åˆå§‹åŒ–å·¥å…·æ¶ˆæ¯å¤„ç†å™¨
        self._tool_message_handler = ToolMessageHandle(
            task_node=self.task_node,
            user_workspace=self._user_workspace,
            tool_call_service=self._tool_call_service,
            event_bus=self._event_bus,
        )

        # ã€å…³é”®ä¿®å¤ã€‘æ¶ˆæ¯IDè¿½è¸ª - ä¸ºæ¯ä¸ªæµå¼æ¶ˆæ¯ç”Ÿæˆå”¯ä¸€ID
        self._current_message_id: str | None = None
        self._message_counter: int = 0

    async def create_task(
        self,
        description: str,
        mode: str = "",
        parent_task_id: str | None = None,
        todos: list[TodoItem] | None = None,
        metadata: dict[str, Any] | None = None,
    ) -> None:
        """åˆ›å»ºä»»åŠ¡ï¼ˆæ›´æ–°å½“å‰ä»»åŠ¡èŠ‚ç‚¹çš„å±æ€§ï¼‰

        Args:
            description: ä»»åŠ¡æè¿°
            mode: ä»»åŠ¡æ¨¡å¼
            parent_task_id: çˆ¶ä»»åŠ¡ID
            todos: å¾…åŠäº‹é¡¹åˆ—è¡¨
            metadata: ä»»åŠ¡å…ƒæ•°æ®

        """
        # æ›´æ–°å½“å‰ä»»åŠ¡èŠ‚ç‚¹çš„å±æ€§
        if description:
            self.task_node.update_description(description)
        if mode:
            self.task_node.update_mode(mode)
        if parent_task_id:
            self.task_node.set_parent(parent_task_id)
        if todos:
            self.task_node.data.todos = todos
        if metadata:
            for key, value in metadata.items():
                self.task_node.add_metadata(key, value)

    async def execute_task(self) -> Any | None:
        """æ‰§è¡Œå½“å‰ä»»åŠ¡èŠ‚ç‚¹

        æ³¨æ„: ä»»åŠ¡æœ¬èº«æ²¡æœ‰æ—¶é—´é™åˆ¶,åªæœ‰LLMè¯·æ±‚æœ‰è¶…æ—¶(é€šè¿‡ process_message ä¸­çš„ llm_timeout æ§åˆ¶)ã€‚
        LLM è¶…æ—¶ç»“åˆ stream_processor ä¸­çš„ idle timeout å®ç°å¿«é€Ÿå¤±è´¥ã€‚

        Returns:
            æ‰§è¡Œç»“æœ

        """
        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹æ—¶é—´
        self.last_checkpoint_time = time.time()

        # åˆ›å»ºæ‰§è¡Œä»»åŠ¡
        self.execution_task = asyncio.create_task(self._run_task_loop())

        result = None  # åˆå§‹åŒ– result å˜é‡ï¼Œé¿å… CancelledError æ—¶æœªå®šä¹‰
        try:
            # ä»»åŠ¡æœ¬èº«æ²¡æœ‰è¶…æ—¶é™åˆ¶,åªé€šè¿‡ LLM è¯·æ±‚è¶…æ—¶æ§åˆ¶
            result = await self.execution_task
        except asyncio.CancelledError:
            # Task cancelled - log info but don't include stack trace (expected flow)
            self.logger.info(f"Task {self.task_node.task_node_id} was cancelled")
            # ä¿å­˜æ£€æŸ¥ç‚¹
            await self._save_checkpoint_on_pause()
        finally:
            # æ¸…ç†
            self.execution_task = None

        return result

    async def _save_checkpoint_on_pause(self) -> None:
        """æš‚åœæ—¶ä¿å­˜æ£€æŸ¥ç‚¹"""
        self.logger.info(f"Saving checkpoint for task {self.task_node.task_node_id} on pause")

        # ä¿å­˜å½“å‰çŠ¶æ€åˆ°æ£€æŸ¥ç‚¹
        task_node = self.task_node
        if task_node:
            state = {
                "task_node_id": task_node.task_node_id,
                "status": "paused",  # æ ‡è®°ä¸ºæš‚åœçŠ¶æ€
                "mode": task_node.mode,
                "todos": [todo.to_dict() for todo in task_node.data.todos],
                "timestamp": time.time(),
            }

            # å‘é€æ£€æŸ¥ç‚¹åˆ›å»ºäº‹ä»¶
            await emit_typed_event(
                TaskEventType.CHECKPOINT_CREATED,
                {
                    "checkpoint_id": f"pause_checkpoint_{task_node.task_node_id}_{int(time.time())}",
                    "reason": "pause",
                    "state": state,
                },
                self._event_bus,
                task_id=task_node.task_node_id,
                source="pause",
            )

            self.logger.info(f"Checkpoint saved successfully for task {task_node.task_node_id}")

    async def pause_task(self) -> bool:
        """æš‚åœä»»åŠ¡æ‰§è¡Œ

        ç«‹å³å–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å¹¶ä¿å­˜æ£€æŸ¥ç‚¹
        """
        if self.execution_task and not self.execution_task.done():
            self.logger.info(f"Cancelling task {self.task_node.task_node_id} for pause")
            self.execution_task.cancel()
            return True
        return False

    async def stream_message_to_event(self, stream_message: StreamMessages) -> None:
        """å°†æµå¼æ¶ˆæ¯è½¬æ¢ä¸ºäº‹ä»¶å¹¶é€šè¿‡äº‹ä»¶æ€»çº¿å‘é€
        æ³¨æ„ï¼šå·¥å…·è°ƒç”¨ç°åœ¨ç”± handle_stream_messages æ–¹æ³•å¤„ç† CompleteMessage ä¸­çš„ tool_calls

        Args:
            stream_message: æµå¼æ¶ˆæ¯

        """
        self.logger.debug(f"Converting stream message to event: {type(stream_message).__name__}")

        # å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯ - è·³è¿‡ï¼Œç°åœ¨å·¥å…·è°ƒç”¨åœ¨ CompleteMessage ä¸­å¤„ç†
        if isinstance(stream_message, ToolCallMessage):
            self.logger.debug(
                "Skipping tool call message in stream_message_to_event, tool calls will be processed in CompleteMessage",
            )
            return
        # å¤„ç†é”™è¯¯æ¶ˆæ¯
        if isinstance(stream_message, ErrorMessage):
            await emit_typed_event(
                TaskEventType.ERROR_OCCURRED,
                stream_message,
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="stream_message",
            )
            return
        # å¤„ç†æ¨ç†æ¶ˆæ¯
        if isinstance(stream_message, ReasoningMessage):
            # ã€å…³é”®ä¿®å¤ã€‘ä¸ºæ–°çš„æ¶ˆæ¯æµç”Ÿæˆmessage_id
            if self._current_message_id is None:
                self._message_counter += 1

                # âœ… ä¼˜å…ˆä½¿ç”¨LLM APIè¿”å›çš„message_idï¼Œå¦‚æœæ²¡æœ‰æ‰ç”Ÿæˆ
                if stream_message.id:
                    self._current_message_id = stream_message.id
                    self.logger.info(
                        f"[MESSAGE_ID] Using LLM API message_id: {self._current_message_id}",
                    )
                else:
                    # Fallback: ç”Ÿæˆä¸´æ—¶ID (å‘åå…¼å®¹)
                    import uuid
                    self._current_message_id = f"msg_{self.task_node.task_node_id}_{self._message_counter}_{uuid.uuid4().hex[:8]}"
                    self.logger.warning(
                        f"[MESSAGE_ID] LLM API did not provide message_id, using generated: {self._current_message_id}",
                    )

            # å‘é€äº‹ä»¶æ—¶é™„å¸¦message_id
            await emit_typed_event(
                TaskEventType.REASONING,
                {
                    "content": stream_message.content,
                    "message_id": self._current_message_id,
                },
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="stream_message",
            )
            return
        # å¤„ç†ä½¿ç”¨ç»Ÿè®¡æ¶ˆæ¯
        if isinstance(stream_message, UsageMessage):
            await emit_typed_event(
                TaskEventType.USAGE_RECEIVED,
                stream_message,
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="stream_message",
            )
            return
        # å¤„ç†å®Œæˆæ¶ˆæ¯
        if isinstance(stream_message, CompleteMessage):
            # å‘é€å®Œæˆäº‹ä»¶ï¼ŒåŒ…å«message_id
            await emit_typed_event(
                TaskEventType.COMPLETE_RECEIVED,
                stream_message,
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="stream_message",
            )

            # é‡ç½®message_idï¼Œä¸ºä¸‹ä¸€æ¡æ¶ˆæ¯åšå‡†å¤‡
            self._current_message_id = None
            # ç§»é™¤returnè¯­å¥ï¼Œè®©å·¥å…·è°ƒç”¨èƒ½å¤Ÿè¢«handle_stream_messageså¤„ç†
            # return  # âŒ åˆ é™¤è¿™è¡Œï¼Œä¿®å¤å·¥å…·è°ƒç”¨ä¸å¤„ç†çš„bug
        # å¤„ç†å†…å®¹æµæ¶ˆæ¯ï¼ˆContentMessageï¼‰
        if isinstance(stream_message, ContentMessage):
            # ã€å…³é”®ä¿®å¤ã€‘ä¸ºæ–°çš„æ¶ˆæ¯æµç”Ÿæˆmessage_id
            if self._current_message_id is None:
                self._message_counter += 1

                # âœ… ä¼˜å…ˆä½¿ç”¨LLM APIè¿”å›çš„message_idï¼Œå¦‚æœæ²¡æœ‰æ‰ç”Ÿæˆ
                if stream_message.id:
                    self._current_message_id = stream_message.id
                    self.logger.info(
                        f"[MESSAGE_ID] Using LLM API message_id: {self._current_message_id}",
                    )
                else:
                    # Fallback: ç”Ÿæˆä¸´æ—¶ID (å‘åå…¼å®¹)
                    import uuid
                    self._current_message_id = f"msg_{self.task_node.task_node_id}_{self._message_counter}_{uuid.uuid4().hex[:8]}"
                    self.logger.warning(
                        f"[MESSAGE_ID] LLM API did not provide message_id, using generated: {self._current_message_id}",
                    )

            # å†…å®¹æ¶ˆæ¯ç›´æ¥å¤„ç†ï¼Œä¸éœ€è¦ç¼“å†²å™¨

            # å‘é€äº‹ä»¶æ—¶é™„å¸¦message_id
            await emit_typed_event(
                TaskEventType.CONTENT_STREAM,
                {
                    "content": stream_message.content,
                    "message_id": self._current_message_id,
                },
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="stream_message",
            )
            return

    async def process_message(
        self,
        _timeout: float = 900.0,
        llm_timeout: float | None = None,
        tool_execution_timeout: float | None = None,
    ) -> Any | None:
        """å¤„ç†æ¶ˆæ¯ï¼Œä½¿ç”¨å›è°ƒæ–¹å¼æé«˜å·¥å…·è°ƒç”¨çš„å®æ—¶æ€§

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            timeout: æ€»è¶…æ—¶æ—¶é—´(ç§’),é»˜è®¤15åˆ†é’Ÿï¼ˆå‘åå…¼å®¹ï¼‰
            llm_timeout: LLMè°ƒç”¨è¶…æ—¶(ç§’),Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
            tool_execution_timeout: å·¥å…·æ‰§è¡Œè¶…æ—¶(ç§’),Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—

        Returns:
            å¤„ç†ç»“æœ

        """
        # ğŸ”§ FIX: æ£€æŸ¥æ˜¯å¦ç”¨æˆ·è¯·æ±‚åœæ­¢ï¼ˆåœ¨å¤„ç†æ¶ˆæ¯å‰ï¼‰
        if self._agent and self._agent.is_stop_requested():
            self.logger.info(
                f"Task {self.task_node.task_node_id} stop requested, raising CancelledError to interrupt LLM",
            )
            raise asyncio.CancelledError("Stop requested by user")

        # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è®¡ç®—è¶…æ—¶
        llm_timeout, tool_execution_timeout = self._get_timeout_for_task(
            llm_timeout=llm_timeout,
            tool_execution_timeout=tool_execution_timeout,
        )

        self._tool_message_handler._has_attempt_completion = False

        self._tool_message_handler._executed_tool_calls.clear()
        self.logger.info("Reset executed tool calls set for new task execution")

        # æ„å»ºAPIè¯·æ±‚
        api_request = await self._message_processor.build_messages(
            user_workspace=self._user_workspace,
            capabilities=self._get_capabilities(),
        )

        if not self._llm_service.get_current_provider():
            # ä½¿ç”¨ LLMProvider è·å–æ¨¡å¼ç‰¹å®šçš„é…ç½®
            mode = self.task_node.mode or "plan"  # é»˜è®¤ä½¿ç”¨ plan æ¨¡å¼
            mode_config = self._user_workspace.llm_manager.get_mode_config(mode)
            if mode_config:
                self._llm_service.set_provider(mode_config.name)

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - é˜²å¾¡æ€§æ£€æŸ¥
        raw_messages = api_request.get("messages", [])
        if not raw_messages:
            self.logger.warning("è­¦å‘Š: æ²¡æœ‰æ¶ˆæ¯è¦å¤„ç†")
            messages = []
        else:
            self.logger.info(f"å¼€å§‹è½¬æ¢ {len(raw_messages)} æ¡æ¶ˆæ¯")
            try:
                converted_messages = []
                for i, msg_dict in enumerate(raw_messages):
                    if msg_dict is None:
                        self.logger.warning(f"æ¶ˆæ¯ {i} ä¸º None,è·³è¿‡")
                        continue
                    if isinstance(msg_dict, dict):
                        # éªŒè¯å¿…è¦çš„ role å­—æ®µ
                        if "role" not in msg_dict:
                            self.logger.error(f"æ¶ˆæ¯ {i} ç¼ºå°‘ role å­—æ®µ: {msg_dict}")
                            raise ValueError(f"æ¶ˆæ¯ {i} ç¼ºå°‘ 'role' å­—æ®µ")

                        # æ ¹æ® role é€‰æ‹©æ­£ç¡®çš„æ¶ˆæ¯ç±»
                        role = msg_dict.get("role")
                        if role == "system":
                            converted_messages.append(SystemMessage.from_openai_format(msg_dict))
                        elif role == "user":
                            converted_messages.append(UserMessage.from_openai_format(msg_dict))
                        elif role == "assistant":
                            converted_messages.append(AssistantMessage.from_openai_format(msg_dict))
                        elif role == "tool":
                            converted_messages.append(ToolMessage.from_openai_format(msg_dict))
                        else:
                            raise ValueError(f"æ¶ˆæ¯ {i} ä¸æ”¯æŒçš„ role: {role}")
                    else:
                        # éå­—å…¸ç±»å‹ç›´æ¥ä¿ç•™
                        converted_messages.append(msg_dict)
                        self.logger.debug(f"æ¶ˆæ¯ {i} ä¿ç•™åŸå§‹ç±»å‹: {type(msg_dict).__name__}")
                messages = converted_messages
            except Exception as e:
                self.logger.error(f"æ¶ˆæ¯æ ¼å¼è½¬æ¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
                # è®°å½•æ‰€æœ‰æ¶ˆæ¯ä¾›è°ƒè¯•
                for i, msg_dict in enumerate(raw_messages):
                    self.logger.exception(f"åŸå§‹æ¶ˆæ¯ {i}: type={type(msg_dict).__name__}, value={repr(msg_dict)[:200]}")
                raise

        async def stream_callback(stream_message: StreamMessages) -> None:
            """æµå¼å›è°ƒå‡½æ•°ï¼Œå¤„ç†å·¥å…·æ¶ˆæ¯å’Œäº‹ä»¶è½¬æ¢"""
            # ğŸ”§ FIX: åœ¨æ¯ä¸ªæµå¼æ¶ˆæ¯å¤„ç†æ—¶æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self._agent and self._agent.is_stop_requested():
                self.logger.info(
                    f"Task {self.task_node.task_node_id} stop requested during streaming, raising CancelledError",
                )
                raise asyncio.CancelledError("Stop requested by user during streaming")

            await self.stream_message_to_event(stream_message)
            if isinstance(stream_message, CompleteMessage):
                # å·¥å…·æ‰§è¡Œä½¿ç”¨ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶
                try:
                    async with asyncio.timeout(tool_execution_timeout):
                        await self._tool_message_handler.handle_stream_messages(stream_message)
                except TimeoutError:
                    # Tool execution timeout - log but don't include stack trace (expected flow)
                    self.logger.exception(f"Tool execution timeout after {tool_execution_timeout}s")
                    # å·¥å…·æ‰§è¡Œè¶…æ—¶ä¸ä¸­æ–­æ•´ä¸ªæµç¨‹ï¼Œç»§ç»­ä¿å­˜checkpoint

        try:
            # LLMè°ƒç”¨ä½¿ç”¨ç‹¬ç«‹çš„è¶…æ—¶æ§åˆ¶
            async with asyncio.timeout(llm_timeout):
                await self._llm_service.create_message_with_callback(
                    messages,
                    callback=stream_callback,
                    tools=api_request.get("tools", []),
                )

            self.logger.info(
                f"Message processed successfully (LLM: {llm_timeout}s, Tools: {tool_execution_timeout}s)",
            )

        except TimeoutError:
            # LLM timeout - log but don't include stack trace (expected flow)
            self.logger.exception(f"LLM call timeout after {llm_timeout}s")
            await self._send_error_to_frontend(
                error_message=f"LLMè°ƒç”¨è¶…æ—¶ï¼ˆ{llm_timeout:.0f}ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–ç®€åŒ–ä»»åŠ¡ã€‚",
                error_type="timeout",
                details={
                    "timeout": llm_timeout,
                    "suggestion": "ç®€åŒ–ä»»åŠ¡å†…å®¹æˆ–ç¨åé‡è¯•",
                },
            )
            raise

        except LLMError as e:
            # LLM API error - log with stack trace for debugging
            error_str = str(e)
            self.logger.error(f"LLM API error: {error_str}", exc_info=True)

            # æ£€æµ‹429 rate limité”™è¯¯
            if "429" in error_str or "rate_limit" in error_str.lower():
                self.logger.warning(f"Rate limit detected: {e}")
                await self._send_error_to_frontend(
                    error_message="APIè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•ã€‚å»ºè®®ç­‰å¾…60ç§’åç‚¹å‡»é‡è¯•æŒ‰é’®ã€‚",
                    error_type="rate_limit_exceeded",
                    details={
                        "error_code": "429",
                        "recoverable": True,
                        "retry_after": 60,
                        "suggestion": "ç­‰å¾…60ç§’åé‡è¯•",
                        "original_error": error_str[:500],  # é™åˆ¶é•¿åº¦
                    },
                )
            else:
                # å…¶ä»–LLMé”™è¯¯
                self.logger.exception("LLM API error: ")
                await self._send_error_to_frontend(
                    error_message=f"LLM APIé”™è¯¯ï¼š{error_str[:200]}",
                    error_type="llm_api_error",
                    details={
                        "error": error_str[:500],
                        "recoverable": False,
                        "suggestion": "è¯·æ£€æŸ¥APIé…ç½®æˆ–è”ç³»ç®¡ç†å‘˜",
                    },
                )
            raise

        except asyncio.CancelledError:
            # ğŸ”§ FIX: ç”¨æˆ·è¯·æ±‚åœæ­¢ - æ­£å¸¸çš„åœæ­¢æµç¨‹,ä¸æ˜¯é”™è¯¯
            self.logger.info(f"Task {self.task_node.task_node_id} was cancelled due to stop request")
            # è®¾ç½®ä»»åŠ¡çŠ¶æ€ä¸ºABORTED
            self.task_node.update_status(TaskStatus.ABORTED)
            raise  # é‡æ–°æŠ›å‡ºä»¥è®©ä¸Šå±‚å¤„ç†

        finally:
            # ä¿å­˜ checkpoint
            await self._save_checkpoint()

    def _get_timeout_for_task(
        self,
        llm_timeout: float | None = None,
        tool_execution_timeout: float | None = None,
    ) -> tuple[float, float]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´

        Args:
            llm_timeout: ç”¨æˆ·æŒ‡å®šçš„LLMè¶…æ—¶
            tool_execution_timeout: ç”¨æˆ·æŒ‡å®šçš„å·¥å…·æ‰§è¡Œè¶…æ—¶

        Returns:
            (llm_timeout, tool_execution_timeout) å…ƒç»„

        """
        # å¦‚æœç”¨æˆ·å·²æ˜ç¡®æŒ‡å®šï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å€¼
        if llm_timeout is not None and tool_execution_timeout is not None:
            return llm_timeout, tool_execution_timeout

        # è·å–ä»»åŠ¡æè¿°ï¼ˆUserInputTextå¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²ï¼‰
        task_description = ""
        if hasattr(self.task_node, "description"):
            desc = self.task_node.description
            if hasattr(desc, "content"):
                # UserInputTextå¯¹è±¡
                task_description = desc.content
            elif isinstance(desc, str):
                # å­—ç¬¦ä¸²
                task_description = desc
            else:
                # å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                task_description = str(desc)

        task_description = task_description.lower()
        # å®‰å…¨è·å– modeï¼ˆå¯èƒ½ä¸º Noneï¼‰
        mode = self.task_node.mode.lower() if self.task_node.mode else ""

        # æ£€æµ‹æ˜¯å¦ä¸ºå¤§å‹å†…å®¹ç”Ÿæˆä»»åŠ¡
        is_large_generation = any(
            keyword in task_description
            for keyword in [
                "html",
                "ç”Ÿæˆhtml",
                "åˆ›å»ºé¡µé¢",
                "build page",
                "generate html",
                "å‰ç«¯",
                "frontend",
                "é¡µé¢è®¾è®¡",
                "page design",
                "ä»£ç ç”Ÿæˆ",
                "code generation",
                "å¤§å‹",
                "large",
            ]
        )

        # æ£€æµ‹æ˜¯å¦ä¸ºå¤æ‚ä»»åŠ¡ï¼ˆåŒ…å«å¤šä¸ªæ­¥éª¤ï¼‰
        is_complex_task = any(
            keyword in task_description
            for keyword in [
                "å¤šä¸ª",
                "multi",
                "æ­¥éª¤",
                "steps",
                "æµç¨‹",
                "workflow",
                "å®Œæ•´",
                "complete",
                "å…¨éƒ¨",
                "all",
                "å…¨é¢",
                "comprehensive",
            ]
        )

        # æ£€æµ‹æ¨¡å¼ç‰¹å®šè¶…æ—¶éœ€æ±‚
        mode_multipliers = {
            "orchestrator": 1.5,  # ç¼–æ’æ¨¡å¼éœ€è¦æ›´é•¿æ—¶é—´
            "architect": 1.2,  # æ¶æ„æ¨¡å¼éœ€è¦æ›´å¤šæ€è€ƒ
            "code": 1.3,  # ä»£ç ç”Ÿæˆå¯èƒ½è¾ƒé•¿
            "ask": 0.8,  # é—®ç­”æ¨¡å¼ç›¸å¯¹å¿«é€Ÿ
            "debug": 1.0,  # è°ƒè¯•æ¨¡å¼æ ‡å‡†æ—¶é—´
        }

        # åŸºç¡€è¶…æ—¶
        base_llm_timeout = 600.0  # 10åˆ†é’Ÿ
        base_tool_timeout = 300.0  # 5åˆ†é’Ÿ

        # åº”ç”¨æ¨¡å¼ä¹˜æ•°
        mode_multiplier = mode_multipliers.get(mode, 1.0)

        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
        if is_large_generation:
            # å¤§å‹å†…å®¹ç”Ÿæˆä»»åŠ¡ï¼šå¢åŠ LLMè¶…æ—¶ï¼Œå‡å°‘å·¥å…·è¶…æ—¶
            calculated_llm_timeout = min(
                base_llm_timeout * 1.5 * mode_multiplier,
                1800.0,
            )  # æœ€å¤š30åˆ†é’Ÿ
            calculated_tool_timeout = base_tool_timeout  # ä¿æŒæ ‡å‡†å·¥å…·è¶…æ—¶
        elif is_complex_task:
            # å¤æ‚ä»»åŠ¡ï¼šä¸¤è€…éƒ½å¢åŠ 
            calculated_llm_timeout = min(
                base_llm_timeout * 1.3 * mode_multiplier,
                1500.0,
            )  # æœ€å¤š25åˆ†é’Ÿ
            calculated_tool_timeout = min(
                base_tool_timeout * 1.5 * mode_multiplier,
                900.0,
            )  # æœ€å¤š15åˆ†é’Ÿ
        else:
            # æ ‡å‡†ä»»åŠ¡
            calculated_llm_timeout = base_llm_timeout * mode_multiplier
            calculated_tool_timeout = base_tool_timeout * mode_multiplier

        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å…¶ä¸­ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªè‡ªåŠ¨è®¡ç®—
        if llm_timeout is not None:
            final_llm_timeout = llm_timeout
            # å·¥å…·è¶…æ—¶çº¦ä¸ºLLMè¶…æ—¶çš„50%
            final_tool_timeout = tool_execution_timeout or (llm_timeout * 0.5)
        elif tool_execution_timeout is not None:
            final_tool_timeout = tool_execution_timeout
            # LLMè¶…æ—¶çº¦ä¸ºå·¥å…·è¶…æ—¶çš„2å€
            final_llm_timeout = llm_timeout or (tool_execution_timeout * 2.0)
        else:
            final_llm_timeout = calculated_llm_timeout
            final_tool_timeout = calculated_tool_timeout

        self.logger.info(
            f"Timeout configuration for task '{self.task_node.mode or 'unknown'}': LLM={final_llm_timeout:.0f}s, Tools={final_tool_timeout:.0f}s (is_large_generation={is_large_generation}, is_complex={is_complex_task})",
        )

        return final_llm_timeout, final_tool_timeout

    async def _save_checkpoint(self) -> None:
        """ä¿å­˜ checkpoint åˆ°ç£ç›˜
        åŒ…å«èŠå¤©æ¶ˆæ¯å†å²ã€ä»»åŠ¡å›¾çŠ¶æ€å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # æ”¶é›†å¯¹è¯æ¶ˆæ¯å†å²
        conversation_messages = []
        if self._user_workspace.current_conversation:
            for msg in self._user_workspace.current_conversation.messages:
                if hasattr(msg, "to_dict"):
                    conversation_messages.append(msg.to_dict())
                else:
                    conversation_messages.append(msg)

        # æ”¶é›†ä»»åŠ¡å›¾çŠ¶æ€
        task_graph_data = {}
        if self._user_workspace.task_graph:
            task_graph_data = {
                "task_id": self._user_workspace.task_graph.task_node_id,
                "nodes": {},
            }
            all_tasks = await self._user_workspace.task_graph.get_all_tasks()
            for task in all_tasks:
                task_graph_data["nodes"][task.task_node_id] = {
                    "task_node_id": task.task_node_id,
                    "description": task.description,
                    "mode": task.mode,
                    "status": task.status.value,
                    "todos": [todo.to_dict() for todo in task.data.todos],
                    "parent_id": task.parent_id,
                    "child_ids": task.child_ids,
                }

        # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_data = {}
        if self._user_workspace.task_graph:
            all_tasks = await self._user_workspace.task_graph.get_all_tasks()
            for task in all_tasks:
                context = task.context.to_dict() if task.context else {}
                context_data[task.task_node_id] = context

        # ä½¿ç”¨æ–°çš„ IntelligentCheckpointManager ä¿å­˜ checkpoint
        # æ„å»ºçŠ¶æ€æ•°æ®
        state = {
            "conversation_messages": conversation_messages,
            "task_graph_data": task_graph_data,
            "context_data": context_data,
            "metadata": {
                "task_node_id": self.task_node.task_node_id,
                "mode": self.task_node.mode,
                "message_count": len(conversation_messages),
            },
        }

        checkpoint_id = await self._checkpoint_manager.create_checkpoint(
            task_id=self.task_node.task_node_id,
            state=state,
            checkpoint_type=CheckpointType.AUTO,  # ä½¿ç”¨è‡ªåŠ¨æ£€æŸ¥ç‚¹ç±»å‹
            tags=["task_node", self.task_node.mode or "unknown"],
        )

        self.logger.info(f"Checkpoint saved: {checkpoint_id}")

        # å®šæœŸæ¸…ç†æ—§ checkpoint
        await self._cleanup_old_checkpoints()

        # å‘é€æ£€æŸ¥ç‚¹åˆ›å»ºäº‹ä»¶
        await emit_typed_event(
            TaskEventType.CHECKPOINT_CREATED,
            {
                "checkpoint_id": checkpoint_id,
                # ä½¿ç”¨å…¨å±€ checkpoints ç›®å½•è·¯å¾„
                "checkpoint_path": f"{get_dawei_home()}/checkpoints/{checkpoint_id}",
                "message_count": len(conversation_messages),
                "task_count": len(task_graph_data.get("nodes", {})),
            },
            self._event_bus,
            task_id=self.task_node.task_node_id,
            source="task_node_executor",
        )

    async def _cleanup_old_checkpoints(self) -> None:
        """å®šæœŸæ¸…ç†æ—§ checkpoint,é˜²æ­¢ç£ç›˜ç©ºé—´è€—å°½

        æ³¨æ„ï¼šIntelligentCheckpointManager ä¼šè‡ªåŠ¨æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹ï¼ˆåœ¨åˆ›å»ºæ–°æ£€æŸ¥ç‚¹æ—¶ï¼‰ï¼Œ
        è¿™é‡Œä¿ç•™å®šæ—¶æ¸…ç†ä½œä¸ºé¢å¤–çš„ä¿éšœæªæ–½ã€‚
        """
        try:
            current_time = time.time()
            # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡ (3600 ç§’)
            cleanup_interval = 3600.0

            if current_time - self.last_cleanup_time > cleanup_interval:
                self._checkpoint_manager.set_max_checkpoints(10)

                # æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡æ¸…ç†ï¼Œé€šè¿‡åˆ—å‡ºå’Œåˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹
                checkpoints = await self._checkpoint_manager.list_checkpoints(
                    task_id=self.task_node.task_node_id,
                )

                # ä¿ç•™æœ€æ–°çš„10ä¸ªï¼Œåˆ é™¤å…¶ä½™çš„
                if len(checkpoints) > 10:
                    for cp_metadata in checkpoints[10:]:
                        await self._checkpoint_manager.delete_checkpoint(cp_metadata.checkpoint_id)

                self.last_cleanup_time = current_time
        except (CheckpointError, OSError) as e:
            self.logger.error(f"Failed to cleanup old checkpoints: {e}", exc_info=True)
            raise  # Fast fail: re-raise cleanup errors

    async def _run_task_loop(self) -> None:
        """è¿è¡Œä»»åŠ¡ä¸»å¾ªç¯
        æŒç»­æ‰§è¡Œç›´åˆ°ä»»åŠ¡çŠ¶æ€ä¸º COMPLETED æˆ– ABORTED
        å®ç°å¤šè½®æ‰§è¡Œæœºåˆ¶ï¼šä¸LLMæŒç»­äº¤äº’ï¼ŒåŠ¨æ€åˆ›å»ºå’Œç»´æŠ¤task graph
        """
        iteration = 0
        max_iterations = 100  # é˜²æ­¢æ— é™å¾ªç¯çš„å®‰å…¨é™åˆ¶

        while iteration < max_iterations:
            iteration += 1

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœå·²å®Œæˆæˆ–ä¸­æ­¢åˆ™é€€å‡ºå¾ªç¯
            status = self.task_node.status
            if status in [TaskStatus.COMPLETED, TaskStatus.ABORTED]:
                self.logger.info(
                    f"Task {self.task_node.task_node_id} completed with status: {status.value}",
                )
                break

            # ğŸ”§ FIX: æ£€æŸ¥æ˜¯å¦ç”¨æˆ·è¯·æ±‚åœæ­¢
            if self._agent and self._agent.is_stop_requested():
                self.logger.info(
                    f"Task {self.task_node.task_node_id} stop requested by user, setting status to ABORTED",
                )
                self.task_node.update_status(TaskStatus.ABORTED)
                break

            await self.process_message()

            if not await self._should_continue_execution():
                # è®¾ç½®ä»»åŠ¡çŠ¶æ€ä¸ºCOMPLETEDï¼Œå› ä¸ºä»»åŠ¡å·²æ­£å¸¸å®Œæˆ
                if self.task_node.status not in [
                    TaskStatus.COMPLETED,
                    TaskStatus.ABORTED,
                ]:
                    self.logger.info(
                        f"Task {self.task_node.task_node_id} finished naturally, setting status to COMPLETED",
                    )
                    self.task_node.update_status(TaskStatus.COMPLETED)
                break

            # çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰å¾…
            await asyncio.sleep(0.05)

        if iteration >= max_iterations:
            self.logger.warning(f"Task {self.task_node.task_node_id} reached max iterations limit")
            self.task_node.update_status(TaskStatus.COMPLETED)

    async def _check_all_todos_completed(self) -> bool:
        """æ£€æŸ¥æ‰€æœ‰å¾…åŠäº‹é¡¹æ˜¯å¦å·²å®Œæˆ

        Returns:
            True if all todos are completed, False otherwise

        """
        if not self.task_node.data.todos:
            return True  # æ²¡æœ‰todosè§†ä¸ºå…¨éƒ¨å®Œæˆ

        return all(todo.status == TodoStatus.COMPLETED for todo in self.task_node.data.todos)

    async def _has_pending_followup_responses(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„followupå“åº”

        Returns:
            True if there are pending followup responses, False otherwise

        """
        try:
            if hasattr(self, "_pending_followup_responses") and self._pending_followup_responses:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„future
                for _tool_call_id, future in self._pending_followup_responses.items():
                    if not future.done():
                        return True
            return False
        except (AttributeError, RuntimeError) as e:
            self.logger.error(f"Error checking pending followup responses: {e}", exc_info=True)
            # Fast fail: re-raise to avoid masking the error
            raise

    async def _should_continue_execution(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­æ‰§è¡Œï¼ˆæ˜¯å¦éœ€è¦æ›´å¤šè½®çš„LLMäº¤äº’ï¼‰

        Returns:
            True if should continue, False otherwise

        """
        if self._tool_message_handler.has_attempt_completion:
            self.logger.info("_has_attempt_completion flag is True, stopping execution")
            return False

        if self._user_workspace.current_conversation and self._user_workspace.current_conversation.messages:
            # æ£€æŸ¥æœ€è¿‘çš„æ¶ˆæ¯
            recent_messages = self._user_workspace.current_conversation.messages[-1:]

            for msg in recent_messages:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if hasattr(tool_call, "function") and tool_call.function.name == "attempt_completion":
                            self.logger.info(
                                "Found attempt_completion tool call, stopping execution",
                            )
                            return False  # å·²ç»è°ƒç”¨äº†attempt_completionï¼Œä¸å†ç»§ç»­

                elif hasattr(msg, "role"):  # æ˜¯ä¸ªassistantï¼Œä½†æ˜¯æ²¡æœ‰tool call
                    if str(msg.role) == str(MessageRole.ASSISTANT):
                        # should complete ?
                        return False
            return True
        return None

    async def _execute_tool_call_todo(self, todo: TodoItem) -> None:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ç±»å‹çš„TODO"""
        if not hasattr(todo, "metadata"):
            self.logger.warning(f"TODO {todo.title} missing metadata for tool call")
            return

        tool_name = todo.metadata.get("tool_name")
        tool_args = todo.metadata.get("tool_args", {})

        if not tool_name:
            self.logger.warning(f"TODO {todo.title} missing tool_name")
            return

        # æ„é€ ToolCallå¯¹è±¡å¹¶æ‰§è¡Œ

        tool_call = ToolCall(id=todo.id, name=tool_name, parameters=tool_args)

        result = await self._tool_message_handler.execute_tool_call(tool_call)
        todo.metadata["result"] = result

    async def _create_subtask_todo(self, todo: TodoItem) -> None:
        """åˆ›å»ºå­ä»»åŠ¡ç±»å‹çš„TODO"""
        if not hasattr(todo, "metadata"):
            self.logger.warning(f"TODO {todo.title} missing metadata for subtask")
            return

        import uuid

        from dawei.task_graph.task_node_data import TaskData

        subtask_id = str(uuid.uuid4())
        description = todo.metadata.get("description", todo.title)
        # è·å– modeï¼Œå¦‚æœçˆ¶ä»»åŠ¡ mode ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤å€¼ "plan"
        parent_mode = self.task_node.mode or "plan"
        mode = todo.metadata.get("mode", parent_mode)

        task_data = TaskData(
            task_node_id=subtask_id,
            description=description,
            mode=mode,
            context=self._user_workspace.create_task_context(),
        )

        await self._user_workspace.task_graph.create_subtask(
            self.task_node.task_node_id,
            task_data,
        )

        todo.metadata["subtask_id"] = subtask_id
        self.logger.info(f"Created subtask {subtask_id} from TODO")

    async def _create_periodic_checkpoint(self) -> None:
        """å®šæœŸåˆ›å»ºæ£€æŸ¥ç‚¹"""
        current_time = time.time()

        if current_time - self.last_checkpoint_time > self._config.checkpoint_interval:
            task_node = self.task_node
            if task_node:
                state = {
                    "task_node_id": task_node.task_node_id,
                    "status": task_node.status.value,
                    "mode": task_node.mode,
                    "todos": [todo.to_dict() for todo in task_node.data.todos],
                    "timestamp": current_time,
                }

                # å‘é€æ£€æŸ¥ç‚¹åˆ›å»ºäº‹ä»¶
                await emit_typed_event(
                    TaskEventType.CHECKPOINT_CREATED,
                    {
                        "checkpoint_id": f"checkpoint_{task_node.task_node_id}_{int(current_time)}",
                        # ä½¿ç”¨å…¨å±€ checkpoints ç›®å½•è·¯å¾„
                        "checkpoint_path": f"{get_dawei_home()}/checkpoints/{task_node.task_node_id}",
                        "checkpoint_size": len(str(state)),
                    },
                    self._event_bus,
                    task_id=task_node.task_node_id,
                    source="periodic_checkpoint",
                )

                self.last_checkpoint_time = current_time

    def _get_capabilities(self) -> list[str]:
        """è·å–èƒ½åŠ›åˆ—è¡¨

        Returns:
            èƒ½åŠ›åˆ—è¡¨

        """
        capabilities = []
        if self._config.enable_skills:
            capabilities.append("Claude Skills integration is enabled")
        if self._config.enable_mcp:
            capabilities.append("MCP (Model Context Protocol) integration is enabled")
        return capabilities

    async def cancel_task_execution(self) -> bool:
        """å–æ¶ˆä»»åŠ¡æ‰§è¡Œ

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ

        """
        if self.execution_task and not self.execution_task.done():
            self.execution_task.cancel()
            self.execution_task = None
            self.logger.info(f"Task execution cancelled: {self.task_node.task_node_id}")
            return True
        return False

    async def is_task_executing(self) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æ­£åœ¨æ‰§è¡Œ

        Returns:
            æ˜¯å¦æ­£åœ¨æ‰§è¡Œ

        """
        return self.execution_task is not None and not self.execution_task.done()

    async def get_execution_status(self) -> dict[str, Any]:
        """è·å–æ‰§è¡ŒçŠ¶æ€

        Returns:
            æ‰§è¡ŒçŠ¶æ€ä¿¡æ¯

        """
        is_executing = await self.is_task_executing()
        last_checkpoint = self.last_checkpoint_time

        return {
            "task_node_id": self.task_node.task_node_id,
            "is_executing": is_executing,
            "last_checkpoint_time": last_checkpoint,
            "execution_time": time.time() - last_checkpoint if last_checkpoint > 0 else 0,
        }

    async def pause_task_execution(self) -> bool:
        """æš‚åœä»»åŠ¡æ‰§è¡Œ

        Returns:
            æ˜¯å¦æˆåŠŸæš‚åœ

        """
        # TODO: Implement task pause functionality
        # Currently, task pause is not fully implemented
        # This method is called by Agent.pause_task() but needs implementation
        self.logger.warning(
            f"pause_task_execution called for task {self.task_node.task_node_id} but not yet implemented",
        )
        return False

    async def resume_task_execution(self) -> bool:
        """æ¢å¤ä»»åŠ¡æ‰§è¡Œ

        Returns:
            æ˜¯å¦æˆåŠŸæ¢å¤

        """
        self.task_node.update_status(TaskStatus.RESUMABLE)
        await self.execute_task()

    async def get_current_mode(self) -> str:
        """è·å–å½“å‰æ¨¡å¼

        Returns:
            å½“å‰æ¨¡å¼

        """
        return self.task_node.mode or "plan"

    async def handle_followup_response(self, tool_call_id: str, response: str) -> bool:
        """å¤„ç†å‰ç«¯å‘æ¥çš„è¿½é—®å›å¤

        Args:
            tool_call_id: å·¥å…·è°ƒç”¨ID
            response: ç”¨æˆ·å›å¤

        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†

        """
        return await self._tool_message_handler.handle_followup_response(tool_call_id, response)

    async def _send_error_to_frontend(
        self,
        error_message: str,
        error_type: str = "execution_error",
        details: dict[str, Any] | None = None,
    ) -> None:
        """å‘é€é”™è¯¯æ¶ˆæ¯åˆ°å‰ç«¯

        Args:
            error_message: é”™è¯¯æ¶ˆæ¯
            error_type: é”™è¯¯ç±»å‹
            details: é”™è¯¯è¯¦æƒ…

        """
        try:
            from dawei.core.events import TaskEventType, emit_typed_event

            # æ„å»ºé”™è¯¯è¯¦æƒ…
            error_details = details or {}
            error_details.update(
                {
                    "task_node_id": self.task_node.task_node_id,
                    "mode": self.task_node.mode,
                    "error_type": error_type,
                },
            )

            # å‘é€é”™è¯¯äº‹ä»¶ï¼ˆä¼šé€šè¿‡chat handlerè½¬å‘åˆ°å‰ç«¯ï¼‰
            self.logger.info(
                f"[ERROR_TRACE] About to emit ERROR_OCCURRED event: error_type={error_type}, task_id={self.task_node.task_node_id}",
            )
            await emit_typed_event(
                TaskEventType.ERROR_OCCURRED,
                {
                    "error_type": error_type,
                    "message": error_message,
                    "details": error_details,
                },
                self._event_bus,
                task_id=self.task_node.task_node_id,
                source="task_node_executor",
            )

            self.logger.info(
                f"[ERROR_TRACE] Successfully emitted ERROR_OCCURRED event: {error_message[:100]}...",
            )

        except (OSError, ConnectionError, RuntimeError) as e:
            self.logger.error(f"Failed to send error to frontend: {e}", exc_info=True)
            # Fast fail: re-raise to avoid masking the original error
            raise
