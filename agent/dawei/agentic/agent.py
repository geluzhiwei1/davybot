# Copyright (c) 2025 æ ¼å¾‹è‡³å¾®
# SPDX-License-Identifier: AGPL-3.0-only

"""é‡æ„åçš„ Agent ä¸»ç±»
é€šè¿‡ TaskGraphExecutionEngine ç®¡ç†æ‰€æœ‰ä»»åŠ¡ç›¸å…³æ“ä½œï¼Œç®€åŒ– Agent èŒè´£
"""

import asyncio
import uuid
from collections import deque
from datetime import UTC, datetime, timezone
from pathlib import Path
from typing import Any

from dawei.core.error_handler import handle_errors
from dawei.core.errors import (
    ConfigurationError,
)
from dawei.core.metrics import increment_counter
from dawei.core.utils import validate_and_create_config
from dawei.entity.task_types import TaskStatus, TaskSummary, TokenUsage, ToolUsage
from dawei.entity.user_input_message import UserInputMessage
from dawei.llm_api.constants import SNAPSHOT
from dawei.llm_api.model_router import (
    ModelRouter,
    load_cost_config,
    load_model_router_config,
)
from dawei.logg.logging import get_logger, log_performance
from dawei.workspace.user_workspace import UserWorkspace

from .context_manager import ContextManager
from .cost_tracker import CostTracker
from .file_reference import FileReferenceParser, PathResolver
from .file_snapshot_manager import FileSnapshotManager
from .task_graph_excutor import TaskGraphExecutionEngine


def is_memory_enabled(user_workspace) -> bool:
    """æ£€æŸ¥å†…å­˜ç³»ç»Ÿæ˜¯å¦å¯ç”¨ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ï¼‰

    Args:
        user_workspace: UserWorkspace instance

    Returns:
        True if memory system is enabled, False otherwise

    Raises:
        RuntimeError: If config is not loaded (Fast Fail)

    Examples:
        >>> ws = UserWorkspace("/path/to/workspace")
        >>> await ws.initialize()
        >>> is_memory_enabled(ws)
        True
    """
    config = user_workspace.get_config()  # Fast Fail if not loaded
    return config.memory.enabled


class Agent:
    """é‡æ„åçš„ Agent ä¸»ç±»

    ä¸“æ³¨äºåè°ƒå„ç»„ä»¶ï¼Œé€šè¿‡ TaskGraphExecutionEngine ç®¡ç†æ‰€æœ‰ä»»åŠ¡æ“ä½œ
    """

    def __init__(
        self,
        user_workspace: UserWorkspace,
        execution_engine: TaskGraphExecutionEngine,
        config: Any | dict[str, Any] | None = None,
    ):
        """åˆå§‹åŒ– Agent
        Args:
            user_workspace: ç”¨æˆ·å·¥ä½œåŒº
            execution_engine: ä»»åŠ¡æ‰§è¡Œå¼•æ“å®ä¾‹
            config: é…ç½®å®ä¾‹æˆ–å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        # åŸºæœ¬å±æ€§
        self.user_workspace = user_workspace
        self.execution_engine = execution_engine

        # æ—¥å¿—è®°å½•å™¨ï¼ˆå¿…é¡»åœ¨ä½¿ç”¨å‰åˆå§‹åŒ–ï¼‰
        self.logger = get_logger(__name__)

        from dawei.core.events import SimpleEventBus
        self.event_bus = SimpleEventBus()
        self.logger.info(
            f"[AGENT] Agent created own event_bus (id={id(self.event_bus)})"
        )

        # ã€å…³é”®ã€‘è®¾ç½®execution_engineçš„agentå¼•ç”¨ï¼Œç”¨äºæš‚åœæ£€æŸ¥
        if hasattr(execution_engine, "_agent"):
            execution_engine._agent = self

        # é…ç½®å¤„ç† - ä½¿ç”¨å…±äº«å·¥å…·å‡½æ•°
        self.config = validate_and_create_config(config)

        # ã€æ–°å¢ã€‘å¤šæ¨¡å‹æ™ºèƒ½è·¯ç”±å™¨ï¼ˆ
        router_config = load_model_router_config(self.user_workspace.absolute_path)
        cost_config = load_cost_config()

        # ã€ä¿®å¤ã€‘è·å–ç”¨æˆ·é€‰æ‹©çš„ LLMï¼Œä¼ é€’ç»™ ModelRouter
        user_default_model = None
        if user_workspace.workspace_info.user_ui_context and user_workspace.workspace_info.user_ui_context.current_llm_id:
            user_default_model = user_workspace.workspace_info.user_ui_context.current_llm_id

        self.model_router = ModelRouter(
            router_config,
            cost_config,
            user_default_model=user_default_model,
        )

        # ã€æ–°å¢ã€‘ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆ
        self.context_manager = ContextManager(max_tokens=200000)

        # ã€æ–°å¢ã€‘æ–‡ä»¶å¼•ç”¨è§£æå™¨ï¼ˆ
        self.file_reference_parser = FileReferenceParser(self.user_workspace.absolute_path)
        self.path_resolver = PathResolver(self.user_workspace.absolute_path)

        # ã€æ–°å¢ã€‘æˆæœ¬è¿½è¸ªå™¨ï¼ˆ
        self.cost_tracker = CostTracker()

        # ã€æ–°å¢ã€‘æ–‡ä»¶å¿«ç…§ç®¡ç†å™¨ï¼ˆ
        self.file_snapshot_manager = FileSnapshotManager(
            workspace_path=str(self.user_workspace.absolute_path),
            max_snapshots_per_file=SNAPSHOT.MAX_SNAPSHOTS_PER_FILE,
            retention_days=SNAPSHOT.RETENTION_DAYS,
            enable_compression=True,
        )

        # ã€æ–°å¢ã€‘å¯¹è¯å‹ç¼©å™¨ï¼ˆé•¿å¯¹è¯Tokené™åˆ¶å¤„ç†ï¼‰
        self.conversation_compressor = None
        self._initialize_compression()

        # ã€æ–°å¢ã€‘Memoryç³»ç»Ÿï¼ˆå¯é€‰ï¼‰- å¿…é¡»åœ¨conversation_memory_integrationä¹‹å‰åˆå§‹åŒ–
        self.memory_graph = None
        self.virtual_context = None
        self.memory_degraded = False  # Track memory system degradation state
        if is_memory_enabled(self.user_workspace):
            try:
                from dawei.memory.memory_graph import MemoryGraph
                from dawei.memory.virtual_context import VirtualContextManager

                workspace_path = str(self.user_workspace.absolute_path)
                memory_db = str(Path(workspace_path) / ".dawei" / "memory.db")

                # Initialize MemoryGraph
                self.memory_graph = MemoryGraph(memory_db, self.event_bus)

                # Initialize VirtualContextManager
                self.virtual_context = VirtualContextManager(
                    db_path=memory_db,
                    page_size=2000,
                    max_active_pages=5,
                )

                self.logger.info("Memory system initialized successfully")
            except Exception as e:
                # Fast Fail: Memory system is explicitly enabled but failed to initialize
                # This indicates a serious problem that should not be silently ignored
                self.logger.error(
                    f"Memory system is enabled but failed to initialize: {e}. System will continue without memory capabilities.",
                    exc_info=True,
                )
                self.memory_degraded = True
                self.memory_graph = None
                self.virtual_context = None

        # ã€æ–°å¢ã€‘å¯¹è¯-è®°å¿†é›†æˆå™¨
        self.conversation_memory_integration = None
        if is_memory_enabled(self.user_workspace):
            from .conversation_memory_integration import ConversationMemoryIntegration

            self.conversation_memory_integration = ConversationMemoryIntegration(
                memory_graph=self.memory_graph,
                virtual_context=self.virtual_context,
                auto_extract=True,
                auto_store=True,
            )
            self.logger.info("Conversation-memory integration initialized")

        # ã€æ–°å¢ã€‘Plan Mode å·¥ä½œæµæ‰§è¡Œå™¨
        self.plan_workflow = None
        if self.config.mode == "plan":
            try:
                from dawei.agentic.plan_workflow import PlanWorkflowExecutor

                self.plan_workflow = PlanWorkflowExecutor(
                    session_id=str(self.user_workspace.uuid),
                    workspace_root=str(self.user_workspace.absolute_path),
                )
                self.logger.info("Plan workflow initialized for plan mode")
            except ImportError as e:
                # Graceful degradation: Plan workflow is optional
                self.logger.warning(f"Failed to initialize plan workflow: {e}", exc_info=True)
                # Plan workflow is optional, continue without it

        # ç»Ÿè®¡ä¿¡æ¯
        self.tool_usage: dict[str, ToolUsage] = {}
        self.token_usage = TokenUsage()
        # ã€å†…å­˜ä¼˜åŒ–ã€‘ä½¿ç”¨ deque é™åˆ¶æ¨¡å‹é€‰æ‹©å†å²è®°å½•æ•°é‡ï¼Œé˜²æ­¢æ— é™å¢é•¿
        self.selected_models: deque = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        self._selected_models_max_age_seconds = 3600  # 1å°æ—¶åæ¸…ç†å†å²è®°å½•

        # ã€ä¿®å¤ã€‘è®°å½•ç”¨æˆ·é€‰æ‹©çš„ LLM æ¨¡å‹ï¼ˆå¦‚æœå·²è®¾ç½®ï¼‰
        if user_default_model:
            self.logger.info(
                f"Agent initialized with user-selected default LLM: {user_default_model}",
            )

        # åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False

        # Agentæ§åˆ¶çŠ¶æ€
        self._stop_requested = False

    @classmethod
    async def create_with_default_engine(
        cls,
        user_workspace: UserWorkspace,
        config: Any | dict[str, Any] | None = None,
    ) -> "Agent":
        """ä½¿ç”¨é»˜è®¤æ‰§è¡Œå¼•æ“åˆ›å»º Agent å®ä¾‹

        Args:
            user_workspace: ç”¨æˆ·å·¥ä½œåŒº
            config: é…ç½®å®ä¾‹æˆ–å­—å…¸ï¼ˆå¯é€‰ï¼‰

        Returns:
            Agent å®ä¾‹

        """
        import logging

        logger = logging.getLogger(__name__)
        logger.info(f"[AGENT_CREATE] Starting agent creation for workspace: {user_workspace.workspace_info.id}")

        # é…ç½®å¤„ç† - ä½¿ç”¨å…±äº«å·¥å…·å‡½æ•°
        config_obj = validate_and_create_config(config)

        # åˆ›å»ºæ‰§è¡Œå¼•æ“ï¼Œä¸ä½¿ç”¨ä¾èµ–æ³¨å…¥å®¹å™¨
        from dawei.llm_api.llm_provider import LLMProvider
        from dawei.prompts import EnhancedSystemBuilder
        from dawei.tools.tool_executor import ToolExecutor
        from dawei.tools.tool_manager import ToolManager
        from dawei.workspace.tool_manager_wrapper import WorkspaceToolManager

        try:
            logger.info("[AGENT_CREATE] Creating base services...")
            # åˆ›å»ºåŸºç¡€æœåŠ¡
            message_processor = EnhancedSystemBuilder(user_workspace=user_workspace)

            if not user_workspace.llm_manager:
                raise RuntimeError("Workspace LLM manager not initialized")
            llm_service = user_workspace.llm_manager
            logger.info("[AGENT_CREATE] Using workspace's LLMProvider")

            # ã€ä¿®å¤ã€‘ä½¿ç”¨ WorkspaceToolManager è€Œä¸æ˜¯ ToolManager
            # WorkspaceToolManager ä¼šåˆå§‹åŒ– Skills å·¥å…·
            workspace_tool_manager = WorkspaceToolManager(workspace_path=user_workspace.absolute_path)
            await workspace_tool_manager.initialize()

            # åˆ›å»º ToolManager ä¼ é€’ç»™ ToolExecutor (å‘åå…¼å®¹)
            tool_manager = ToolManager(workspace_path=user_workspace.absolute_path)

            tool_call_service = ToolExecutor(
                tool_manager=tool_manager,
            )

            # ã€æ–°å¢ã€‘å°† WorkspaceToolManager çš„ skills å·¥å…·æ·»åŠ åˆ° ToolExecutor
            if workspace_tool_manager._skills_tools:
                for skill_tool in workspace_tool_manager._skills_tools:
                    tool_call_service.tools[skill_tool.name] = skill_tool
                    logger.info(f"[AGENT_CREATE] Loaded skill tool: {skill_tool.name}")

            # Initialize task manager callbacks (must be done before setting user workspace)
            await tool_call_service.async_initialize()

            # è®¾ç½®å·¥å…·æ‰§è¡Œå™¨çš„å·¥ä½œåŒº
            await tool_call_service.set_user_workspace(user_workspace)

            logger.info("[AGENT_CREATE] Creating Agent instance (first, to get event_bus)...")
            # ğŸ”§ ä¿®å¤ï¼šå…ˆåˆ›å»º Agent å®ä¾‹(è·å– event_bus)
            agent = cls(user_workspace, None, config_obj)

            logger.info("[AGENT_CREATE] Creating TaskGraph for workspace...")
            # ğŸ”§ ä¿®å¤ï¼šåˆ›å»º TaskGraph å¹¶ä½¿ç”¨ Agent çš„ event_bus
            from dawei.task_graph.task_graph import TaskGraph
            task_id = user_workspace.workspace_info.id if user_workspace.workspace_info else "default-task"
            user_workspace.task_graph = TaskGraph(
                task_id=task_id,
                event_bus=agent.event_bus  # ä½¿ç”¨ Agent çš„ event_bus
            )

            logger.info("[AGENT_CREATE] Creating execution engine...")
            # åˆ›å»ºæ‰§è¡Œå¼•æ“,ä¼ å…¥ agent
            execution_engine = TaskGraphExecutionEngine(
                user_workspace=user_workspace,
                message_processor=message_processor,
                llm_service=llm_service,
                tool_call_service=tool_call_service,
                config=config_obj,
                agent=agent,  # ğŸ”§ ä¿®å¤ï¼šä¼ å…¥ agent
            )

            # ğŸ”§ ä¿®å¤ï¼šè®¾ç½® Agent çš„ execution_engine
            agent.execution_engine = execution_engine

            # ã€æ–°å¢ã€‘è®¾ç½® ToolExecutor çš„ agent å¼•ç”¨ï¼ˆç”¨äºæƒé™æ£€æŸ¥ï¼‰
            tool_call_service._agent = agent

            # è®¾ç½® event_busï¼ˆç”¨äºå‘é€å·¥å…·äº‹ä»¶ï¼‰
            tool_call_service._event_bus = agent.event_bus

            logger.info("[AGENT_CREATE] Agent created successfully")
            return agent
        except Exception as e:
            logger.error(f"[AGENT_CREATE] Error during agent creation: {e}", exc_info=True)
            raise

    @handle_errors(component="agent", operation="initialize")
    @log_performance("agent.initialize")
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Agent

        Returns:
            æ˜¯å¦æˆåŠŸåˆå§‹åŒ–

        """
        # éªŒè¯é…ç½® - ç›´æ¥æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¿”å›False
        config_errors = self.config.validate()
        if config_errors:
            raise ConfigurationError(f"Configuration validation failed: {config_errors}")

        # åˆå§‹åŒ–æ‰§è¡Œå¼•æ“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(self.execution_engine, "initialize"):
            await self.execution_engine.initialize()

        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
        self._initialized = True

        self.logger.info("Agent initialized", context={"component": "agent"})
        increment_counter("agent.initialization", tags={"status": "success"})
        return True

    @handle_errors(component="agent", operation="process_message")
    @log_performance("agent.process_message")
    async def process_message(self, message: UserInputMessage) -> Any:
        """å¤„ç†æ¶ˆæ¯
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
        Returns:
            å¤„ç†ç»“æœ
        """
        # è·å–æ¶ˆæ¯å†…å®¹
        prompt = message.content if hasattr(message, "content") else str(message)

        # ã€æ–°å¢ã€‘æ–‡ä»¶å¼•ç”¨è§£æï¼ˆ
        parsed_refs = self.file_reference_parser.parse(prompt)
        if parsed_refs.references:
            # è§£ææ–‡ä»¶è·¯å¾„
            parsed_refs = self.path_resolver.resolve_all(parsed_refs)

            # ã€æ–°å¢ã€‘å¤„ç†æŠ€èƒ½å¼•ç”¨
            from dawei.agentic.file_reference import ReferenceType

            skill_references = [ref for ref in parsed_refs.references if ref.reference_type == ReferenceType.SKILL and ref.is_valid]

            if skill_references:
                # åŠ è½½æŠ€èƒ½å†…å®¹åˆ°ä¸Šä¸‹æ–‡
                from pathlib import Path

                from dawei.tools.skill_manager import SkillManager

                # æ„å»ºæŠ€èƒ½ç®¡ç†å™¨ï¼ŒåŒ…å«å·¥ä½œåŒºå’Œç”¨æˆ·æŠ€èƒ½ç›®å½•
                skills_roots = [Path(self.user_workspace.absolute_path), Path.home()]
                skill_manager = SkillManager(skills_roots=skills_roots)
                skill_manager.discover_skills(force=True)

                skills_loaded = 0
                for skill_ref in skill_references:
                    # æå–æŠ€èƒ½åç§°ï¼ˆä» "@skill:xxx" æˆ– "skill:xxx" ä¸­æå– "xxx"ï¼‰
                    raw_path = skill_ref.raw_path
                    skill_name = raw_path.lstrip("@").replace("skill:", "")

                    # è·å–æŠ€èƒ½å†…å®¹
                    skill_content = skill_manager.get_skill_content(skill_name)

                    if skill_content:
                        # å°†æŠ€èƒ½å†…å®¹æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
                        self.context_manager.add_skill_context(skill_name, skill_content)
                        skills_loaded += 1
                        self.logger.info(
                            f"âœ… Loaded skill '{skill_name}' to context ({len(skill_content)} chars)",
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸  Skill '{skill_name}' not found or could not be loaded",
                        )

                if skills_loaded > 0:
                    self.logger.info(f"âœ… Successfully loaded {skills_loaded} skill(s) to context")

                    # å‘å¸ƒæŠ€èƒ½å¼•ç”¨äº‹ä»¶
                    from dawei.core.events import TaskEventType

                    await self.event_bus.publish(
                        TaskEventType.SKILLS_LOADED,
                        {
                            "skills": [skill_ref.raw_path.replace("skill:", "") for skill_ref in skill_references],
                            "total_skills": skills_loaded,
                        },
                    )

            # è‡ªåŠ¨æ·»åŠ æ–‡ä»¶åˆ°ä¸Šä¸‹æ–‡
            files_added = 0
            for ref in parsed_refs.references:
                if ref.is_valid and ref.reference_type != ReferenceType.SKILL:
                    for file_path in ref.resolved_paths:
                        self.context_manager.add_file(file_path)
                        files_added += 1
                        self.logger.debug(f"Added file to context: {file_path}")

            if files_added > 0:
                self.logger.info(f"Added {files_added} file(s) to context from user message")

                # å‘å¸ƒæ–‡ä»¶å¼•ç”¨äº‹ä»¶
                from dawei.core.events import TaskEventType

                await self.event_bus.publish(
                    TaskEventType.FILES_REFERENCED,
                    {
                        "files": [
                            {
                                "path": ref.resolved_paths,
                                "type": ref.reference_type.value,
                            }
                            for ref in parsed_refs.references
                            if ref.is_valid and ref.reference_type != ReferenceType.SKILL
                        ],
                        "total_files": files_added,
                    },
                )

            # ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼ˆç§»é™¤æ–‡ä»¶å¼•ç”¨æ ‡è®°ï¼‰
            prompt = parsed_refs.cleaned_message
            # æ›´æ–°æ¶ˆæ¯å†…å®¹
            if hasattr(message, "content"):
                message.content = prompt

        # ã€æ–°å¢ã€‘æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼ˆ
        context_length = self._estimate_context_length()

        # åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®ä»»åŠ¡
        is_critical = self._is_critical_task(prompt)

        # é€‰æ‹©æœ€ä¼˜æ¨¡å‹
        model_selection = self.model_router.select_model(
            prompt=prompt,
            context_length=context_length,
            is_critical=is_critical,
        )

        # è®°å½•æ¨¡å‹é€‰æ‹©
        self.selected_models.append(
            {
                "model": model_selection.model,
                "reason": model_selection.reason,
                "task_type": model_selection.task_type.value,
                "timestamp": model_selection.timestamp,
                "context_length": context_length,
            },
        )

        # å‘å¸ƒæ¨¡å‹é€‰æ‹©äº‹ä»¶
        from dawei.core.events import TaskEventType

        await self.event_bus.publish(
            TaskEventType.MODEL_SELECTED,
            {
                "model": model_selection.model,
                "reason": model_selection.reason,
                "task_type": model_selection.task_type.value,
                "confidence": model_selection.confidence,
                "context_length": context_length,
                "is_critical": is_critical,
            },
        )

        self.logger.info(
            f"Selected model: {model_selection.model} (reason: {model_selection.reason}, task: {model_selection.task_type.value})",
        )

        # Plan Mode: Attach plan_workflow to user_workspace context
        if self.plan_workflow is not None:
            self.user_workspace._plan_workflow = self.plan_workflow
            self.logger.debug("Plan workflow attached to user_workspace context")
        else:
            self.user_workspace._plan_workflow = None

        # ç»§ç»­å¤„ç†æ¶ˆæ¯
        await self.execution_engine.process_message(message)
        await self.execution_engine.execute_task_graph()

        # ã€æ–°å¢ã€‘æ‰§è¡Œå®Œæˆåè‡ªåŠ¨æå–è®°å¿†
        # Fast fail: only catch expected errors, let unknown errors propagate
        if self.memory_graph is not None and not self.memory_degraded:
            try:
                await self._extract_memories_after_execution()
            except (ConnectionError, TimeoutError, OSError) as e:
                # Expected errors: network issues, timeouts, etc.
                self.logger.warning(f"Memory extraction failed due to connectivity issue: {e}")
            except Exception as e:
                # Unexpected errors should fail fast - log as error
                self.logger.error(f"Memory extraction failed unexpectedly: {e}", exc_info=True)
                raise

        return

    async def _extract_memories_after_execution(self) -> None:
        """ä»æœ€è¿‘å¯¹è¯ä¸­æå–ç»“æ„åŒ–è®°å¿†

        åœ¨ Agent æ‰§è¡Œå®Œæˆåè‡ªåŠ¨è°ƒç”¨ï¼Œä»å¯¹è¯ä¸­æå–ï¼š
        - äº‹å®è®°å¿† (facts)
        - åå¥½è®°å¿† (preferences)
        - ç­–ç•¥è®°å¿† (strategies)
        """
        import uuid

        # è·å–å¯¹è¯æ¶ˆæ¯
        if not hasattr(self.user_workspace, "current_conversation"):
            return

        conversation = self.user_workspace.current_conversation
        if not conversation or not hasattr(conversation, "messages"):
            return

        messages = conversation.messages
        if len(messages) < 2:
            return  # éœ€è¦è‡³å°‘ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤

        # è·å–æœ€è¿‘çš„æ¶ˆæ¯è¿›è¡Œè®°å¿†æå–
        recent_messages = messages[-10:]  # æœ€å¤šå–æœ€è¿‘10æ¡

        # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        messages_text = "\n".join([f"{str(msg.role)}: {msg.content[:500] if hasattr(msg, 'content') and msg.content else ''}" for msg in recent_messages if hasattr(msg, "content") and msg.content])

        if not messages_text.strip():
            return

        # ä½¿ç”¨ LLM æå–ç»“æ„åŒ–è®°å¿†
        try:
            llm_service = self.execution_engine._llm_service

            extraction_prompt = f"""ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ç»“æ„åŒ–äº‹å®ã€‚

è¦æ±‚ï¼š
1. æ¯è¡Œä¸€ä¸ªäº‹å®ï¼Œæ ¼å¼ä¸ºï¼š[ä¸»ä½“] [å…³ç³»] [å¯¹è±¡]
2. åªæå–é‡è¦çš„äº‹å®ã€åå¥½å’Œç”¨æˆ·ä¿¡æ¯
3. å…³ç³»ä½¿ç”¨è‹±æ–‡åŠ¨è¯æˆ–çŸ­è¯­ï¼šprefers, likes, uses, knows, works_on ç­‰

ç¤ºä¾‹ï¼š
- User prefers Python
- Project uses FastAPI
- User dislikes JavaScript
- User works_on software development

å¯¹è¯ï¼š
{messages_text}

æå–çš„äº‹å®ï¼š"""

            # Convert dict format to LLMMessage format
            from dawei.entity.lm_messages import UserMessage
            llm_messages = [UserMessage(role="user", content=extraction_prompt)]

            response = await llm_service.process_message(
                messages=llm_messages,
                max_tokens=500,
                temperature=0.3,
            )

            if not response or not response.get("content"):
                return

            # è§£æå¹¶å­˜å‚¨è®°å¿†
            from dawei.memory.memory_graph import MemoryEntry, MemoryType

            for line in response["content"].strip().split("\n"):
                line = line.strip()
                if not line or line.startswith("-"):
                    line = line.lstrip("-").strip()

                parts = line.split(maxsplit=2)
                if len(parts) >= 3:
                    # æ¨æ–­è®°å¿†ç±»å‹
                    memory_type = MemoryType.FACT
                    predicate_lower = parts[1].lower()
                    if any(w in predicate_lower for w in ["prefers", "likes", "loves", "wants"]):
                        memory_type = MemoryType.PREFERENCE
                    elif any(w in predicate_lower for w in ["uses", "requires", "needs"]):
                        memory_type = MemoryType.PROCEDURE
                    elif any(w in predicate_lower for w in ["learned", "discovered"]):
                        memory_type = MemoryType.STRATEGY

                    # æå–å…³é”®è¯
                    keywords = []
                    for part in parts:
                        import re

                        keywords.extend(re.findall(r"\b[A-Z][a-z]+\b", part))

                    memory = MemoryEntry(
                        id=str(uuid.uuid4()),
                        subject=parts[0],
                        predicate=parts[1],
                        object=parts[2],
                        valid_start=datetime.now(UTC),
                        memory_type=memory_type,
                        confidence=0.7,
                        energy=1.0,
                        keywords=list(set(keywords))[:5],
                        metadata={
                            "source": "conversation_extraction",
                            "conversation_id": str(conversation.id),
                        },
                    )

                    await self.memory_graph.add_memory(memory)
                    self.logger.debug(f"Extracted memory: {parts[0]} {parts[1]} {parts[2]}")

            self.logger.info(f"Memory extraction completed for conversation {conversation.id}")

        except (ConnectionError, TimeoutError, OSError) as e:
            # Expected: network/connectivity issues - fall back to simple extraction
            self.logger.warning(f"LLM unavailable for memory extraction, using fallback: {e}")
            await self._extract_memories_simple()
        except Exception as e:
            # Unexpected: should fail fast - log as error with stack trace
            self.logger.error(f"LLM-based memory extraction failed unexpectedly: {e}", exc_info=True)
            # Still fall back to simple extraction but surface the error
            await self._extract_memories_simple()

    async def _extract_memories_simple(self) -> None:
        """ç®€å•çš„è®°å¿†æå–ï¼ˆä¸ä¾èµ– LLMï¼‰

        ä½¿ç”¨è§„åˆ™åŒ¹é…ä»å¯¹è¯ä¸­æå–åŸºæœ¬ä¿¡æ¯
        """
        import re
        import uuid

        if not hasattr(self.user_workspace, "current_conversation"):
            return

        conversation = self.user_workspace.current_conversation
        if not conversation or not hasattr(conversation, "messages"):
            return

        messages = conversation.messages
        if len(messages) < 2:
            return

        from dawei.memory.memory_graph import MemoryEntry, MemoryType

        # ç®€å•æ¨¡å¼åŒ¹é…
        patterns = {
            r"æˆ‘å–œæ¬¢(.+)": ("User", "prefers", MemoryType.PREFERENCE),
            r"æˆ‘åå¥½(.+)": ("User", "prefers", MemoryType.PREFERENCE),
            r"æˆ‘åœ¨åš(.+)": ("User", "works_on", MemoryType.CONTEXT),
            r"æˆ‘çš„é¡¹ç›®æ˜¯(.+)": ("Project", "is", MemoryType.FACT),
        }

        extracted_count = 0
        for msg in messages[-5:]:
            if not hasattr(msg, "content") or not msg.content:
                continue

            content = msg.content
            if str(msg.role) != "user":
                continue

            for pattern, (subject, predicate, mem_type) in patterns.items():
                matches = re.findall(pattern, content)
                for match in matches:
                    memory = MemoryEntry(
                        id=str(uuid.uuid4()),
                        subject=subject,
                        predicate=predicate,
                        object=match.strip(),
                        valid_start=datetime.now(UTC),
                        memory_type=mem_type,
                        confidence=0.5,
                        energy=1.0,
                        keywords=[subject, predicate],
                        metadata={"source": "simple_extraction"},
                    )
                    await self.memory_graph.add_memory(memory)
                    extracted_count += 1

        if extracted_count > 0:
            self.logger.info(f"Simple extraction: {extracted_count} memories")

    def _estimate_context_length(self) -> int:
        """ä¼°ç®—å½“å‰ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä½¿ç”¨ ContextManagerï¼‰"""
        stats = self.context_manager.get_stats()
        return stats.used

    def _is_critical_task(self, prompt: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®ä»»åŠ¡"""
        critical_keywords = [
            "formal report",
            "æ­£å¼æŠ¥å‘Š",
            "production",
            "ç”Ÿäº§ç¯å¢ƒ",
            "official",
            "å®˜æ–¹",
            "critical",
            "é‡è¦",
        ]
        prompt_lower = prompt.lower()
        return any(kw in prompt_lower for kw in critical_keywords)

    @handle_errors(component="agent", operation="pause_task")
    @log_performance("agent.pause_task")
    async def pause_task(self, task_id: str) -> bool:
        """æš‚åœä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸæš‚åœ

        """
        # å§”æ‰˜ç»™æ‰§è¡Œå¼•æ“
        result = await self.execution_engine.pause_task_execution(task_id)

        self.logger.info("Task paused", context={"task_id": task_id, "component": "agent"})
        increment_counter("agent.tasks_paused", tags={"status": "success"})
        return result

    @handle_errors(component="agent", operation="resume_task")
    @log_performance("agent.resume_task")
    async def resume_task(self, task_id: str) -> bool:
        """æ¢å¤ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸæ¢å¤

        """
        # å§”æ‰˜ç»™æ‰§è¡Œå¼•æ“
        result = await self.execution_engine.resume_task_execution(task_id)

        self.logger.info("Task resumed", context={"task_id": task_id, "component": "agent"})
        increment_counter("agent.tasks_resumed", tags={"status": "success"})
        return result

    @handle_errors(component="agent", operation="abort_task")
    @log_performance("agent.abort_task")
    async def abort_task(self, task_id: str, reason: str | None = None) -> bool:
        """ä¸­æ­¢ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            reason: ä¸­æ­¢åŸå› ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸä¸­æ­¢

        """
        # å§”æ‰˜ç»™æ‰§è¡Œå¼•æ“
        result = await self.execution_engine.cancel_task_execution(task_id)

        self.logger.info(
            "Task aborted",
            context={"task_id": task_id, "reason": reason, "component": "agent"},
        )
        increment_counter("agent.tasks_aborted", tags={"status": "success"})
        return result

    @handle_errors(component="agent", operation="complete_task")
    @log_performance("agent.complete_task")
    async def complete_task(self, task_id: str, result: str | None = None) -> bool:
        """å®Œæˆä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            result: ä»»åŠ¡ç»“æœï¼ˆå¯é€‰ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆ

        """
        status_info = await self.execution_engine.get_task_execution_status(task_id)

        # å¦‚æœä»»åŠ¡æ­£åœ¨æ‰§è¡Œï¼Œç­‰å¾…å…¶å®Œæˆ
        if status_info.get("is_executing", False):
            self.logger.info(f"Task {task_id} is still executing, waiting for completion")

        # è®°å½•ä»»åŠ¡ç»“æœ
        if result:
            self.logger.info(f"Task {task_id} completed with result: {result}")

        increment_counter("agent.tasks_completed", tags={"status": "success"})
        return True

    @handle_errors(component="agent", operation="get_task_status")
    async def get_task_status(self, task_id: str) -> TaskStatus:
        """è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡çŠ¶æ€

        """
        status_info = await self.execution_engine.get_task_execution_status(task_id)

        # æ ¹æ®çŠ¶æ€ä¿¡æ¯è¿”å›ç›¸åº”çš„TaskStatus
        if status_info.get("is_executing", False):
            return TaskStatus.RUNNING
        if status_info.get("execution_time", 0) > 0:
            return TaskStatus.COMPLETED
        return TaskStatus.PENDING

    @handle_errors(component="agent", operation="get_task_summary")
    async def get_task_summary(self, task_id: str) -> TaskSummary:
        """è·å–ä»»åŠ¡æ‘˜è¦

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡æ‘˜è¦

        """
        # TaskGraphExecutionEngine needs to get task info through task graph
        task = None  # Requires more complex logic
        mode_history = await self.get_mode_history(task_id)

        # Calculate number of subtasks created
        subtasks_created = 0  # Subtask statistics to be implemented

        return TaskSummary(
            task_id=task_id,
            instance_id="",  # To be retrieved from configuration
            initial_mode=task.mode if task else "",
            final_mode=task.mode if task else "",
            mode_transitions=len(mode_history),
            skill_calls=0,  # Skill call statistics to be implemented
            mcp_requests=0,  # MCP request statistics to be implemented
            subtasks_created=subtasks_created,
            tool_usage={name: {"attempts": usage.attempts, "failures": usage.failures} for name, usage in self.tool_usage.items()},
            token_usage={
                "input_tokens": self.token_usage.input_tokens,
                "output_tokens": self.token_usage.output_tokens,
                "cache_write_tokens": self.token_usage.cache_write_tokens,
                "cache_read_tokens": self.token_usage.cache_read_tokens,
                "total_cost": self.token_usage.total_cost,
            },
        )

    @handle_errors(component="agent", operation="get_task_statistics")
    async def get_task_statistics(self, task_id: str) -> dict[str, Any]:
        """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯

        """
        # TaskGraphExecutionEngine needs to get task info through task graph
        task = None  # Requires more complex logic
        current_mode = await self.get_current_mode(task_id)
        mode_history = await self.get_mode_history(task_id)

        # Get execution status from execution engine
        execution_status = await self.execution_engine.get_task_execution_status(task_id)

        # Simplify message counting logic
        messages_count = 0
        if hasattr(self.user_workspace, "current_conversation") and self.user_workspace.current_conversation and hasattr(self.user_workspace.current_conversation, "messages"):
            messages_count = len(self.user_workspace.current_conversation.messages)

        return {
            "task_id": task_id,
            "instance_id": "",  # To be retrieved from configuration
            "status": (await self.get_task_status(task_id)).value,
            "initial_mode": task.mode if task else "",
            "current_mode": current_mode,
            "messages_count": messages_count,
            "tool_usage": {name: {"attempts": usage.attempts, "failures": usage.failures} for name, usage in self.tool_usage.items()},
            "token_usage": {
                "input_tokens": self.token_usage.input_tokens,
                "output_tokens": self.token_usage.output_tokens,
                "cache_write_tokens": self.token_usage.cache_write_tokens,
                "cache_read_tokens": self.token_usage.cache_read_tokens,
                "total_cost": self.token_usage.total_cost,
            },
            "mode_transitions": len(mode_history),
            "execution_status": execution_status,
        }

    def _cleanup_selected_models(self) -> int:
        """æ¸…ç†è¿‡æœŸçš„æ¨¡å‹é€‰æ‹©è®°å½•

        Returns:
            æ¸…ç†çš„è®°å½•æ•°é‡
        """
        if not self.selected_models:
            return 0

        cutoff_time = datetime.now(UTC).timestamp() - self._selected_models_max_age_seconds
        original_len = len(self.selected_models)

        # è¿‡æ»¤æ‰è¶…æ—¶çš„è®°å½•
        self.selected_models = deque(
            (item for item in self.selected_models if item.get("timestamp", 0) > cutoff_time),
            maxlen=1000,
        )

        cleaned = original_len - len(self.selected_models)
        if cleaned > 0:
            self.logger.debug(f"Cleaned {cleaned} expired model selection records")

        return cleaned

    @handle_errors(component="agent", operation="cleanup")
    @log_performance("agent.cleanup")
    async def cleanup(self, _task_id: str | None = None) -> bool:
        """æ¸…ç†èµ„æº

        Args:
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç†

        """
        # å®šä¹‰éœ€è¦æ¸…ç†çš„ç»„ä»¶åˆ—è¡¨
        cleanup_targets = [
            ("execution_engine", None),
            ("_llm_service", "_http_session"),
            ("_tool_call_service", None),
            ("_event_bus", None),
            ("_message_processor", None),
        ]

        # æ¸…ç†æ‰§è¡Œå¼•æ“
        await self.execution_engine.cleanup()

        # æ¸…ç†å„ä¸ªç»„ä»¶
        for attr_name, session_attr in cleanup_targets:
            component = getattr(self.execution_engine, attr_name, None)
            if component:
                # æ¸…ç†ä¸»ç»„ä»¶
                if hasattr(component, "cleanup"):
                    await component.cleanup()

                # å…³é—­HTTPä¼šè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if session_attr and hasattr(component, session_attr):
                    session = getattr(component, session_attr)
                    if session:
                        await session.close()
                        setattr(component, session_attr, None)

        # æ¸…ç†ä»»åŠ¡å›¾
        task_graph = getattr(self.execution_engine, "task_graph", None)
        if task_graph and hasattr(task_graph, "cleanup"):
            await task_graph.cleanup()

        # æ¸…ç†ç”¨æˆ·å·¥ä½œåŒº
        if self.user_workspace and hasattr(self.user_workspace, "cleanup"):
            await self.user_workspace.cleanup()

        # æ¸…ç†æ¨¡å‹é€‰æ‹©å†å²è®°å½•
        self._cleanup_selected_models()

        # é‡ç½®åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False

        self.logger.info("Agent cleaned up successfully", context={"component": "agent"})
        increment_counter("agent.cleanup", tags={"status": "success"})

        return True  # âœ… Return True to indicate successful cleanup

    async def stop(self) -> str:
        """åœæ­¢Agentæ‰§è¡Œ

        Returns:
            str: ç»“æœæ‘˜è¦

        """
        self.logger.info("Stopping agent execution", context={"component": "agent"})

        # è®¾ç½®åœæ­¢æ ‡å¿—
        self._stop_requested = True

        # å°è¯•åœæ­¢æ‰§è¡Œå¼•æ“
        result_summary = "Agentå·²åœæ­¢"
        try:
            if hasattr(self.execution_engine, "stop"):
                result_summary = await self.execution_engine.stop()
            else:
                result_summary = "Agentæ‰§è¡Œå·²åœæ­¢"
        except RuntimeError as e:
            # Expected error during shutdown
            self.logger.warning(f"Execution engine shutdown warning: {e}", exc_info=True)
            result_summary = f"Agentå·²åœæ­¢: {e!s}"
        except Exception as e:
            # Unexpected errors should be logged but not crash the stop process
            self.logger.error(f"Error stopping execution engine: {e}", exc_info=True)
            result_summary = f"Agentåœæ­¢æ—¶å‘ç”Ÿé”™è¯¯: {e!s}"
            # Re-raise to ensure caller is aware of the failure
            raise

        increment_counter("agent.stop", tags={"status": "success"})
        return result_summary

    def is_stop_requested(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¯·æ±‚åœæ­¢"""
        return self._stop_requested

    # ========================================================================
    # Memory System Methods
    # ========================================================================

    async def add_memory(
        self,
        subject: str,
        predicate: str,
        object_: str,
        memory_type: str = "fact",
        confidence: float = 0.8,
        energy: float = 1.0,
        keywords: list[str] | None = None,
    ) -> str | None:
        """Add a memory entry

        Args:
            subject: Subject entity
            predicate: Relationship/property
            object_: Object entity
            memory_type: Type of memory (fact, preference, strategy, episodic)
            confidence: Confidence score (0-1)
            energy: Energy score (0-1)
            keywords: Optional keywords for retrieval

        Returns:
            Memory ID if successful, None otherwise

        """
        if not self.memory_graph:
            self.logger.debug("Memory system not enabled, skipping add_memory")
            return None

        try:
            from dawei.memory.memory_graph import MemoryEntry, MemoryType

            memory = MemoryEntry(
                id=str(uuid.uuid4()),
                subject=subject,
                predicate=predicate,
                object=object_,
                valid_start=datetime.now(UTC),
                memory_type=MemoryType(memory_type),
                confidence=confidence,
                energy=energy,
                keywords=keywords or [],
                metadata={"source": "agent"},
            )

            memory_id = await self.memory_graph.add_memory(memory)
            self.logger.info(f"Memory added: {memory_id} ({subject} {predicate} {object_})")
            return memory_id
        except Exception as e:
            self.logger.error(f"Failed to add memory: {e}", exc_info=True)
            return None

    async def query_memory(
        self,
        subject: str | None = None,
        predicate: str | None = None,
        object_: str | None = None,
        memory_type: str | None = None,
        only_valid: bool = True,
    ) -> list[Any]:
        """Query memories

        Args:
            subject: Filter by subject
            predicate: Filter by predicate
            object_: Filter by object
            memory_type: Filter by memory type
            only_valid: Only return currently valid memories

        Returns:
            List of memory entries

        """
        if not self.memory_graph:
            return []

        try:
            from dawei.memory.memory_graph import MemoryType

            mem_type = MemoryType(memory_type) if memory_type else None

            return await self.memory_graph.query_temporal(
                subject=subject,
                predicate=predicate,
                object=object_,
                memory_type=mem_type,
                only_valid=only_valid,
            )
        except Exception as e:
            self.logger.error(f"Failed to query memory: {e}", exc_info=True)
            return []

    async def search_memories(self, query: str, limit: int = 50) -> list[Any]:
        """Search memories by keyword

        Args:
            query: Search query
            limit: Maximum number of results

        Returns:
            List of matching memories

        """
        if not self.memory_graph:
            return []

        try:
            return await self.memory_graph.search_memories(query, limit=limit)
        except Exception as e:
            self.logger.error(f"Failed to search memories: {e}", exc_info=True)
            return []

    async def get_memory_stats(self) -> dict[str, Any] | None:
        """Get memory statistics

        Returns:
            Memory statistics dict or None

        """
        if not self.memory_graph:
            return None

        try:
            stats = await self.memory_graph.get_stats()
            return {
                "total": stats.total,
                "by_type": stats.by_type,
                "avg_confidence": stats.avg_confidence,
                "avg_energy": stats.avg_energy,
            }
        except Exception as e:
            self.logger.error(f"Failed to get memory stats: {e}", exc_info=True)
            # Re-raise to ensure caller is aware of the failure
            raise

    async def extract_memories_from_conversation(self) -> dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘ä»å½“å‰å¯¹è¯ä¸­æå–è®°å¿†

        Returns:
            æå–ç»“æœå­—å…¸ï¼ŒåŒ…å«æå–çš„è®°å¿†æ•°é‡

        """
        if not self.memory_graph:
            return {"extracted": 0, "error": "Memory system not enabled"}

        try:
            # è°ƒç”¨å†…éƒ¨æå–æ–¹æ³•
            await self._extract_memories_after_execution()
            return {"extracted": 1, "message": "Memory extraction triggered"}
        except Exception as e:
            self.logger.error(f"Failed to extract memories: {e}", exc_info=True)
            return {"extracted": 0, "error": str(e)}

    async def retrieve_associative_memories(
        self,
        entities: list[str],
        hops: int = 1,
        min_energy: float = 0.2,
    ) -> list[Any]:
        """Retrieve memories via associative graph traversal

        Args:
            entities: List of entity names to start from
            hops: Number of hops to traverse
            min_energy: Minimum energy threshold

        Returns:
            List of related memories

        """
        if not self.memory_graph:
            return []

        try:
            return await self.memory_graph.retrieve_associative(
                query_entities=entities,
                hops=hops,
                min_energy=min_energy,
            )
        except Exception as e:
            self.logger.error(f"Failed to retrieve associative memories: {e}", exc_info=True)
            return []

    # ========================================================================
    # Conversation Compression Methods
    # ========================================================================

    def _initialize_compression(self):
        """åˆå§‹åŒ–å¯¹è¯å‹ç¼©å™¨

        ä»é…ç½®ç³»ç»Ÿè¯»å–å‹ç¼©é…ç½®ï¼Œè€Œä¸æ˜¯ç›´æ¥è¯»å–ç¯å¢ƒå˜é‡ã€‚
        æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶ã€ç¯å¢ƒå˜é‡ç­‰æ–¹å¼é…ç½®ã€‚
        """
        try:
            from dawei.config.settings import get_settings

            from .conversation_compressor import ConversationCompressor

            settings = get_settings()
            compression_config = settings.compression

            if not compression_config.enabled:
                self.logger.debug("Conversation compression disabled by config")
                return

            self.conversation_compressor = ConversationCompressor(
                context_manager=self.context_manager,
                preserve_recent=compression_config.preserve_recent,
                max_tokens=compression_config.max_tokens,
                compression_threshold=compression_config.compression_threshold,
                aggressive_threshold=compression_config.aggressive_threshold,
            )

            self.logger.info(
                f"Conversation compressor initialized: preserve_recent={compression_config.preserve_recent}, max_tokens={compression_config.max_tokens}, thresholds={compression_config.compression_threshold:.0%}-{compression_config.aggressive_threshold:.0%}",
            )

        except Exception as e:
            self.logger.warning(f"Failed to initialize conversation compressor: {e}", exc_info=True)
            self.conversation_compressor = None

    def get_compression_config(self) -> dict[str, Any] | None:
        """è·å–å½“å‰å‹ç¼©é…ç½®

        Returns:
            å‹ç¼©é…ç½®å­—å…¸ï¼Œå¦‚æœæœªå¯ç”¨åˆ™è¿”å›None

        """
        if not self.conversation_compressor:
            return None

        try:
            from dawei.config.settings import get_settings

            settings = get_settings()
            compression_config = settings.compression

            return {
                "enabled": compression_config.enabled,
                "preserve_recent": compression_config.preserve_recent,
                "max_tokens": compression_config.max_tokens,
                "compression_threshold": compression_config.compression_threshold,
                "aggressive_threshold": compression_config.aggressive_threshold,
                "page_size": compression_config.page_size,
                "max_active_pages": compression_config.max_active_pages,
                "memory_integration_enabled": compression_config.memory_integration_enabled,
            }
        except Exception as e:
            self.logger.error(f"Failed to get compression config: {e}", exc_info=True)
            return None

    async def reload_compression_config(self):
        """é‡æ–°åŠ è½½å‹ç¼©é…ç½®

        å½“é…ç½®æ–‡ä»¶å˜æ›´æ—¶è°ƒç”¨æ­¤æ–¹æ³•é‡æ–°åˆå§‹åŒ–å‹ç¼©å™¨ã€‚
        """
        self.logger.info("Reloading compression configuration...")
        self.conversation_compressor = None
        self._initialize_compression()
